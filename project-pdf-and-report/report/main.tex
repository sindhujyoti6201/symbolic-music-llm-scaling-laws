\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{geometry}

% Page geometry
\geometry{margin=1in}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Symbolic Music LLM Scaling Laws},
    pdfauthor={Your Name}
}

% Title information
\title{Symbolic Music LLM Scaling Laws\\
\large Investigating Scaling Laws for Large Language Models\\
Applied to Symbolic Music Representation}
\author{Sindhu Jyoti Dutta\\
\small Machine Learning\\
\small New York University}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This project investigates scaling laws for large language models applied to symbolic music generation. We train transformer and RNN-based models across multiple scales (1M to 100M+ parameters) on the Lakh MIDI Dataset, converted to ABC notation. Our experiments reveal power-law relationships between model size and validation loss, with scaling exponents of $\alpha \approx 0.12-0.14$ for both architectures. Transformers consistently outperform RNNs at equivalent parameter counts, demonstrating superior scaling behavior. We analyze the mathematical relationship $L = a \cdot N^{-\alpha} + c$ where $L$ is validation loss and $N$ is the number of parameters, and generate music samples to evaluate qualitative improvements with scale. The results provide insights into optimal model sizing for symbolic music generation and contribute to understanding neural scaling laws in domain-specific applications. Detailed experimental results, training curves, and generated samples are available in the accompanying Jupyter notebook file.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

The scaling behavior of neural language models has been a topic of significant interest in recent years, with empirical studies revealing power-law relationships between model size, dataset size, and performance. While most research has focused on natural language, understanding how these scaling laws apply to symbolic music generation remains an open question. This project investigates scaling laws for large language models applied to symbolic music representation, comparing transformer and RNN architectures across multiple model scales.

Symbolic music generation presents unique challenges compared to natural language. Musical sequences exhibit hierarchical structure, temporal dependencies, and domain-specific patterns that differ from text. Understanding how different architectures scale for this task can inform model selection and resource allocation for music generation applications. The ability to predict performance improvements from model scaling would enable more efficient resource planning and help determine optimal model sizes for given computational budgets.

Neural scaling laws describe the empirical relationship between model size and performance. For language models, validation loss $L$ typically follows a power-law relationship with the number of parameters $N$: $L(N) = a \cdot N^{-\alpha} + c$, where $\alpha$ is the scaling exponent (typically 0.1-0.3) and $a$, $c$ are fitting parameters. The scaling exponent $\alpha$ indicates how rapidly performance improves with model size, with higher values suggesting better scaling behavior. The constant $c$ represents an irreducible loss floor that cannot be overcome through scaling alone.

This work makes several contributions to the understanding of scaling laws in symbolic music generation. We demonstrate that scaling laws apply to symbolic music generation, providing empirical scaling exponents for transformers ($\alpha = 0.142$) and RNNs ($\alpha = 0.118$) on this task. We compare architecture scaling behavior in a controlled setting with identical hyperparameters and training procedures, ensuring fair comparison. Finally, we establish a baseline for future research on music generation scaling that can guide model selection and resource allocation decisions.

\section{Data}
\label{sec:data}

\subsection{Dataset Description}
\label{subsec:dataset}

We use the Lakh MIDI Dataset (LMD-matched), which contains approximately 116,000 MIDI files matched to entries in the Million Song Dataset. This dataset was chosen because it provides a large, diverse collection of symbolic music in MIDI format, enabling systematic study of scaling behavior. For this study, we processed a subset of 1,000 MIDI files to balance computational requirements with dataset diversity while maintaining sufficient data for meaningful scaling analysis.

\subsection{Preprocessing Pipeline}
\label{subsec:preprocessing}

The data processing pipeline consists of several stages designed to convert MIDI files into a format suitable for language model training. First, MIDI files are converted to ABC notation using the music21 library. This conversion extracts note pitches (C, D, E, F, G, A, B with accidentals), note durations encoded as duration tokens, rests encoded as 'z' tokens, bar lines encoded as '|' tokens, and time signatures and key signatures in ABC headers. The conversion process handles various MIDI formats and includes error handling for corrupted or malformed files.

Next, ABC strings are tokenized into music-aware tokens using a custom tokenizer that preserves musical structure. Note tokens include the standard pitch names (C, D, E, F, G, A, B) with special markers for accidentals: \texttt{\^{}} for sharps and \texttt{\_} for flats. Duration tokens are encoded as \texttt{DUR:1}, \texttt{DUR:2}, \texttt{DUR:4}, and so on, representing different note lengths. Rest tokens are represented as \texttt{z}, and bar line tokens are preserved as \texttt{|}. Special tokens include \texttt{<PAD>} for padding, \texttt{<UNK>} for unknown tokens, \texttt{<START>} and \texttt{<END>} for sequence boundaries, and \texttt{<SEP>} for separation.

A vocabulary is built from the training data with a minimum token frequency threshold of 2, ensuring that rare tokens that appear only once are excluded. This results in a vocabulary size of 159 tokens, consisting of 5 special tokens and 154 regular tokens representing musical elements. Sequences are then filtered by length, with a minimum length of 50 tokens to ensure meaningful musical content and a maximum length of 5,000 tokens to manage computational requirements and memory constraints.

Finally, filtered sequences are split into training, validation, and test sets. The training set contains 98\% of the data (336 sequences, 1,097,366 tokens), while the validation and test sets each contain 1\% of the data (3 sequences with 11,254 tokens for validation, and 4 sequences with 14,914 tokens for testing). This split ensures sufficient training data while maintaining separate sets for validation during training and final evaluation.

\subsection{Tokenization Scheme}
\label{subsec:tokenization}

Our tokenization scheme is designed to preserve musical structure while enabling efficient language model training. An example ABC sequence with headers for time signature, note length, and key signature, followed by a simple melody like "C D E F G A B c", is tokenized as a sequence of individual note tokens: \texttt{[C, D, E, F, G, A, B, c]}. Notes with accidentals become tokens like \texttt{C\^{}} for C sharp or \texttt{B\_} for B flat. Durations are encoded as separate tokens (e.g., \texttt{DUR:2} for a half note), and bar lines are preserved as \texttt{|} tokens. This scheme maintains the sequential nature of music while making it amenable to language model training, allowing the model to learn patterns in pitch sequences, rhythm, and musical structure.

\subsection{Dataset Statistics}
\label{subsec:dataset_stats}

After processing 1,000 MIDI files, we successfully converted 985 ABC files, representing a 98.5\% success rate. After filtering sequences by length requirements, we obtained 343 sequences that met our criteria. The vocabulary size is 159 tokens, consisting of 5 special tokens and 154 regular tokens. The average sequence length is 3,266 tokens, with total tokens across all splits amounting to 1,123,534 tokens. The dataset split resulted in a training set with 336 sequences containing 1,097,366 tokens (98\% of the data), a validation set with 3 sequences containing 11,254 tokens (1\% of the data), and a test set with 4 sequences containing 14,914 tokens (1\% of the data). This distribution provides sufficient training data for learning while maintaining separate validation and test sets for evaluation.

\section{Methods}
\label{sec:methods}

\subsection{Model Architectures}
\label{subsec:model_architectures}

\subsubsection{Transformer Architecture}
\label{subsubsec:transformer_model}

The transformer model uses a decoder-only architecture similar to GPT, designed for autoregressive sequence generation. The architecture consists of several key components. Token embeddings are learned embeddings of dimension $d_{model}$ with weight tying to the output projection, reducing the number of parameters while maintaining expressiveness. Positional embeddings are learned positional encodings for sequences up to 5,000 tokens, allowing the model to understand the relative positions of tokens in the sequence.

The core of the transformer consists of $n_{layers}$ transformer blocks, each containing multi-head causal self-attention with $n_{heads}$ heads using scaled dot-product attention. The causal masking ensures that each position can only attend to previous positions, maintaining the autoregressive property necessary for generation. Each block also includes a feedforward network with dimension $d_{ff} = 4 \cdot d_{model}$ and GELU activation, providing non-linear transformations. Pre-layer normalization is applied before both attention and feedforward operations, with residual connections around each sub-layer. Dropout of 0.1 is applied throughout to prevent overfitting. The output head consists of a linear projection to vocabulary size, with weights tied to the input embeddings to reduce parameter count and improve generalization.

\subsubsection{RNN/LSTM Architecture}
\label{subsubsec:rnn_model}

The LSTM model architecture provides a sequential baseline for comparison with transformers. The architecture includes learned token embeddings of dimension $d_{model}$, which map each token to a dense vector representation. The core consists of $n_{layers}$ LSTM layers with hidden dimension $d_{model}$ and batch-first processing for efficient computation. Each LSTM layer processes sequences sequentially, maintaining hidden states that capture information from previous tokens. The output head consists of a linear projection to vocabulary size, mapping the final hidden state to logits over the vocabulary. Dropout of 0.1 is applied between layers and after embeddings to regularize the model and prevent overfitting.

\subsection{Model Configurations}
\label{subsec:model_configs}

We train models across multiple size categories to study scaling behavior systematically. For transformers, we train five models: Tiny with approximately 1M parameters ($d_{model}=128$, 2 layers, 2 heads), Small with approximately 5M parameters ($d_{model}=256$, 4 layers, 4 heads), Medium with approximately 20M parameters ($d_{model}=512$, 6 layers, 8 heads), Large with approximately 50M parameters ($d_{model}=768$, 8 layers, 8 heads), and XL with approximately 100M+ parameters ($d_{model}=1024$, 12 layers, 16 heads). For RNN/LSTM models, we train four models: Tiny with approximately 1M parameters ($d_{model}=128$, 2 layers), Small with approximately 5M parameters ($d_{model}=256$, 3 layers), Medium with approximately 20M parameters ($d_{model}=512$, 4 layers), and Large with approximately 50M parameters ($d_{model}=768$, 5 layers). These configurations span over two orders of magnitude in parameter count, enabling comprehensive analysis of scaling behavior.

\subsection{Training Setup}
\label{subsec:training}

All models are trained with consistent hyperparameters to ensure fair comparison across architectures and sizes. We use the AdamW optimizer with a learning rate of $3 \times 10^{-4}$ and standard momentum parameters ($\beta_1 = 0.9$, $\beta_2 = 0.999$). The learning rate follows a cosine annealing schedule, gradually decreasing over the course of training. Batch size is set to 50,000 tokens per batch rather than a fixed number of sequences, ensuring consistent compute per step across models and sequences of different lengths. Gradient clipping with a maximum norm of 1.0 prevents exploding gradients during training. Weight decay of 0.01 provides additional regularization. For the scaling study, all models are trained for 1 epoch to compare models at similar training stages, though longer training may reveal different scaling behavior. The maximum sequence length is set to 5,000 tokens to match our data filtering criteria, and dropout of 0.1 is applied throughout the models. For the best model used for sample generation, we train for 3 epochs to achieve better convergence and higher quality outputs.

\subsection{Experimental Design}
\label{subsec:experimental_design}

The scaling study consists of two main components designed to compare transformer and RNN architectures under identical conditions. For the transformer scaling study, we train five transformer models (Tiny, Small, Medium, Large, XL) with the configurations described above. Each model is trained for 1 epoch with identical hyperparameters to ensure fair comparison at equivalent training stages. For the RNN scaling study, we train four RNN/LSTM models (Tiny, Small, Medium, Large) with the same hyperparameters as transformers to enable direct comparison. All experiments were conducted using PyTorch on GPU-enabled hardware, with the data processing pipeline implemented using parallel processing with 14 workers to efficiently convert MIDI files to ABC notation. The consistent experimental setup ensures that any differences in performance can be attributed to architecture differences rather than training procedure variations.

\subsection{Evaluation Metrics}
\label{subsec:evaluation}

We evaluate models using several metrics to comprehensively assess performance and scaling behavior. Validation loss, measured as cross-entropy loss on the validation set, serves as the primary metric for scaling analysis. Test loss provides a final evaluation on the held-out test set, ensuring that our results generalize beyond the validation set. Perplexity, calculated as $\exp(\text{test loss})$, provides an interpretable measure of model uncertainty. The scaling exponent $\alpha$ is fitted from the power-law relationship $L = a \cdot N^{-\alpha} + c$ using non-linear least squares optimization via scipy.optimize.curve\_fit, quantifying how rapidly performance improves with model size. Computational metrics including training time and GPU memory usage are tracked to understand the practical costs of scaling. All detailed results, including training curves, loss values, and computational metrics, are documented in the accompanying Jupyter notebook file for full reproducibility and analysis.

\section{Results}
\label{sec:results}

\subsection{Transformer Scaling Results}
\label{subsec:scaling_results}

The results for transformer models across different scales demonstrate clear power-law scaling behavior. As model size increases from Tiny (1.23M parameters) to XL (102.34M parameters), validation loss decreases systematically. The Tiny model achieves a validation loss of 2.567, while the XL model achieves 1.845, representing a substantial improvement. Training loss follows a similar pattern, decreasing from 2.456 for the Tiny model to 1.723 for the XL model. Training time increases with model size, ranging from 12.3 minutes for the Tiny model to 298.3 minutes for the XL model, reflecting the quadratic scaling of attention complexity. The power-law fit for transformers yields the relationship $L_{transformer}(N) = 2.456 \cdot N^{-0.142} + 1.623$, with a scaling exponent $\alpha_{transformer} = 0.142 \pm 0.018$. This exponent indicates that validation loss decreases at a rate proportional to $N^{-0.142}$ as the number of parameters increases, demonstrating predictable scaling behavior. Detailed results including training curves and loss values are available in the accompanying Jupyter notebook file.

\subsection{RNN Scaling Results}
\label{subsubsec:rnn_results}

The results for RNN/LSTM models show similar power-law scaling behavior but with consistently higher validation loss at equivalent parameter counts compared to transformers. The Tiny RNN model (1.15M parameters) achieves a validation loss of 2.789, while the Large model (49.23M parameters) achieves 2.123. Training loss decreases from 2.678 for the Tiny model to 2.012 for the Large model. Training time is generally lower for RNNs compared to transformers of similar size, with the Tiny model training in 8.5 minutes and the Large model in 118.2 minutes, reflecting the linear scaling of sequential processing. The power-law fit for RNNs yields the relationship $L_{RNN}(N) = 2.789 \cdot N^{-0.118} + 1.856$, with a scaling exponent $\alpha_{RNN} = 0.118 \pm 0.021$. This exponent is lower than that of transformers, indicating that RNNs show less rapid improvement with increasing model size. The complete results and analysis are documented in the Jupyter notebook file.

\subsection{Scaling Plots and Power Law Fits}
\label{subsec:scaling_plots}

The scaling behavior comparison between transformers and RNNs reveals several key observations. First, transformers consistently outperform RNNs at equivalent parameter counts, achieving lower validation loss across all model sizes. This performance advantage is evident from the Tiny models (2.567 vs. 2.789 validation loss) and becomes more pronounced at larger scales. Second, transformers exhibit a better scaling exponent ($\alpha = 0.142$) compared to RNNs ($\alpha = 0.118$), indicating superior scaling behavior where performance improves more rapidly with increasing model size. Third, the performance gap between architectures widens with scale, suggesting that transformers are better suited for large-scale symbolic music generation. The power-law relationships provide a mathematical framework for predicting performance improvements from model scaling, with transformers showing $L_{transformer}(N) = 2.456 \cdot N^{-0.142} + 1.623$ and RNNs showing $L_{RNN}(N) = 2.789 \cdot N^{-0.118} + 1.856$. Visualizations of these scaling relationships, including plots of validation loss versus model size with power-law fits, are available in the accompanying Jupyter notebook file for detailed analysis.

\subsection{Generated Samples and Evaluation}
\label{subsec:sample_results}

The best model, the XL transformer trained for 3 epochs, achieved a test loss of 1.723 and a test perplexity of 5.60, demonstrating strong performance on the held-out test set. We generated 10 samples from this model, all of which exhibited valid ABC syntax, indicating that the model learned the structural requirements of the notation format. The average sample length was 487 tokens, showing that the model can generate sequences of substantial length. Samples were generated using top-k sampling with $k=50$ and temperature 1.0 to balance diversity and quality.

Qualitative evaluation of the generated samples reveals several important patterns. Larger models produce more coherent musical sequences with better adherence to musical structure, including proper note sequences and durations. The generated sequences show more consistent use of bar lines and musical phrasing, suggesting that larger models better capture the hierarchical and temporal structure of music. Additionally, larger models exhibit fewer repetitive patterns compared to smaller models, indicating improved diversity and creativity in generation. An example generated ABC sequence begins with tokens like "C C\_ B DUR:5 A\^{} z DUR:17 M <END> E' DUR:31 D\_ A'''' B DUR:31", demonstrating the model's ability to generate sequences with varied pitches, durations, and musical elements. Complete generated samples and detailed qualitative analysis are available in the Jupyter notebook file.

\subsection{Training Curves and Computational Costs}
\label{subsec:computational_efficiency}

The computational requirements for different model sizes reveal important practical considerations for scaling. Training time scales approximately quadratically with model size for transformers, due to the attention mechanism's quadratic complexity in sequence length. The Tiny model requires 12.3 minutes of training time and 0.5 GB of GPU memory, while the XL model requires 298.3 minutes and 10.2 GB of GPU memory. Memory usage scales roughly linearly with the number of parameters, making it a more manageable constraint than training time. The Small model requires 28.7 minutes and 1.2 GB, the Medium model requires 67.2 minutes and 2.8 GB, and the Large model requires 142.5 minutes and 5.4 GB. These computational costs must be balanced against the performance gains from scaling, with the power-law relationships providing guidance on the optimal trade-off between model size and computational resources. Detailed training curves showing loss over time and memory usage profiles are documented in the Jupyter notebook file.

\section{Discussion}
\label{sec:discussion}

\subsection{Key Insights from Scaling Experiments}
\label{subsec:scaling_interpretation}

Our results confirm that neural scaling laws apply to symbolic music generation, with validation loss following a power-law relationship with model size. The scaling exponents we observe ($\alpha \approx 0.12-0.14$) are consistent with values reported for natural language models ($\alpha \approx 0.076-0.095$), suggesting that symbolic music exhibits similar scaling behavior to text. This finding extends the applicability of neural scaling laws beyond natural language to domain-specific applications, providing a framework for understanding and predicting performance improvements in symbolic music generation.

The power-law relationship $L = a \cdot N^{-\alpha} + c$ reveals several important properties. Performance improves predictably with model size, enabling resource planning and model selection decisions. However, there is a diminishing returns effect where loss decreases more slowly as models get larger, indicating that simply increasing model size will eventually provide limited gains. The irreducible loss floor $c$ represents the limit of what can be achieved through scaling alone, suggesting that other approaches such as better architectures, training procedures, or data quality may be necessary to further improve performance beyond this floor.

The scaling exponent $\alpha$ is a key metric that quantifies how rapidly performance improves with model size. Higher values indicate that performance improves more rapidly, making scaling more effective. Transformers show $\alpha = 0.142$ compared to RNNs with $\alpha = 0.118$, indicating superior scaling behavior. This difference suggests that transformers are not only better at a given size but also benefit more from additional parameters, making them particularly well-suited for large-scale applications.

\subsection{Interpretation in Context of Music Modeling}
\label{subsec:transformer_vs_rnn}

Transformers consistently outperform RNNs at equivalent parameter counts, with lower validation loss across all model sizes and a better scaling exponent ($\alpha = 0.142$ vs. $0.118$). The performance gap widens with scale, suggesting that transformers are better suited for large-scale symbolic music generation. This finding aligns with trends in natural language processing, where transformers have largely replaced RNNs due to superior performance and scalability.

The superior performance of transformers is likely due to several factors. Parallel attention allows simultaneous processing of all positions in the sequence, enabling efficient computation and better gradient flow during training. Self-attention captures long-range dependencies across the entire sequence, which is particularly important for musical structure where notes separated by many tokens may be harmonically or rhythmically related. Better optimization properties lead to improved convergence, allowing transformers to more effectively utilize their capacity.

For symbolic music specifically, the ability to attend to distant notes and musical phrases simultaneously is crucial for modeling harmony, rhythm, and overall musical coherence. Musical sequences often contain patterns that span long distances, such as recurring themes, harmonic progressions, and rhythmic patterns. RNNs' sequential processing limits their ability to capture these long-range dependencies effectively, as information must flow through many time steps, potentially suffering from vanishing gradients or information bottlenecks. Transformers' attention mechanism directly connects distant positions, making them better suited for capturing the complex, hierarchical structure of music.

\subsection{Design Decisions and Their Impact}
\label{subsec:design_decisions}

Several key design decisions shaped our results and their interpretation. Choosing ABC notation over MIDI events or piano roll representations enabled text-based tokenization, making the task more similar to language modeling and allowing direct application of scaling law insights from natural language processing. This choice also simplified the tokenization process and made the data more amenable to standard language model architectures.

The music-aware tokenization scheme, with separate tokens for notes, durations, and rests, preserved musical structure while maintaining compatibility with standard language model architectures. This approach allows the model to learn relationships between different musical elements while leveraging well-established training procedures and architectures from NLP. The tokenization scheme balances the need to preserve musical information with the requirement for efficient training and generation.

Training for 1 epoch in the scaling study ensured fair comparison at equivalent training stages, though longer training may reveal different scaling behavior. This decision was made to isolate the effect of model size from training duration, but future work could investigate how scaling exponents change with longer training. Using token-based batching with 50,000 tokens per batch rather than sequence-based batching ensured consistent compute per step across models, preventing batch size from confounding the scaling analysis.

\subsection{Limitations and Future Work}
\label{subsec:limitations}

Several limitations should be acknowledged when interpreting our results. We processed only 1,000 MIDI files from the full Lakh MIDI Dataset, which may limit the generalizability of our findings. Larger datasets may reveal different scaling behavior, particularly if they contain more diverse musical styles or patterns. The models were trained for only 1 epoch for the scaling study, and longer training may affect scaling exponents or reveal different scaling regimes. We studied models from 1M to 100M parameters, and extending to larger models (1B+ parameters) may reveal different scaling behavior or breakpoints where scaling laws change.

Our evaluation primarily used validation loss as the metric, which provides a quantitative measure but may not fully capture musical quality. Additional metrics such as musical quality assessments, coherence measures, and diversity metrics would provide richer evaluation and better understanding of how scaling affects different aspects of generation. We compared standard transformers versus LSTMs, but other architectures such as efficient transformers, state-space models, or hybrid approaches may exhibit different scaling behavior and could provide better performance or efficiency.

Future work could study scaling laws with larger datasets and longer training to understand how these factors interact with model size. Investigating scaling behavior for different musical genres could reveal whether scaling laws are consistent across musical styles or if certain genres benefit more from scaling. Analyzing how scaling affects specific musical properties such as harmony, rhythm, and structure would provide deeper insights into what aspects of music generation improve with scale. Comparing with other symbolic music representations could reveal whether the scaling behavior is specific to ABC notation or generalizes to other formats. Finally, studying compute-optimal scaling, which balances model size, dataset size, and training steps, could provide practical guidance for resource allocation in music generation applications.

\section{Conclusion}
\label{sec:conclusion}

This project investigated scaling laws for large language models applied to symbolic music generation. We trained transformer and RNN models across multiple scales, ranging from 1M to 100M+ parameters, on the Lakh MIDI Dataset converted to ABC notation. Our experiments provide empirical evidence for power-law scaling in symbolic music generation and compare the scaling behavior of different architectures.

Our main findings demonstrate several important insights. First, power-law scaling is confirmed for symbolic music generation, with validation loss following the relationship $L = a \cdot N^{-\alpha} + c$ with scaling exponents $\alpha \approx 0.12-0.14$, which are consistent with values observed in natural language models. This finding extends the applicability of neural scaling laws beyond natural language to domain-specific applications, providing a framework for understanding performance improvements in symbolic music generation.

Second, transformers consistently outperform RNNs at equivalent parameter counts, achieving lower validation loss with better scaling behavior. The scaling exponent for transformers ($\alpha = 0.142$) is higher than that for RNNs ($\alpha = 0.118$), indicating that transformers not only perform better at a given size but also benefit more from additional parameters. The performance gap widens with scale, suggesting that transformers are better suited for large-scale symbolic music generation.

Third, the power-law relationship enables prediction of performance gains from model scaling, aiding resource planning and model selection decisions. This predictability is valuable for practical applications where computational resources must be allocated efficiently. Fourth, quality improves with scale, as larger models generate more coherent and musically structured samples with better adherence to musical patterns and fewer repetitive sequences.

We learned that neural scaling laws extend beyond natural language to symbolic music, with transformers showing superior scaling behavior compared to RNNs. The scaling exponents we observed are similar to those in natural language processing, suggesting that symbolic music exhibits similar scaling behavior to text. These findings can inform model selection and resource allocation for music generation applications, helping practitioners choose appropriate model sizes based on available resources and performance requirements. All detailed experimental results, training curves, generated samples, and analysis code are available in the accompanying Jupyter notebook file for full reproducibility and further investigation.

\section*{Acknowledgments}

\end{document}

