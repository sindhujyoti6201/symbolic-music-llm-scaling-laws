{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Symbolic Music LLM Scaling Laws - Complete Pipeline\n",
        "\n",
        "This notebook contains the complete pipeline for:\n",
        "1. Data collection and preprocessing (10,000 MIDI files → ABC notation)\n",
        "2. Tokenization and train/val/test splitting\n",
        "3. Training transformer models for scaling laws study\n",
        "\n",
        "**Note**: This notebook is designed for Google Colab. Make sure to enable GPU runtime (Runtime → Change runtime type → GPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q music21 pretty_midi librosa mir_eval numpy pandas scipy torch transformers datasets matplotlib seaborn tqdm pyyaml wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import random\n",
        "import gc\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "# Aggressively suppress all warnings, especially from music21\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "import music21\n",
        "music21.environment.UserSettings()['warnings'] = 0\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Setup paths for Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    BASE_DIR = Path('/content/symbolic-music-llm')\n",
        "    BASE_DIR.mkdir(exist_ok=True)\n",
        "    os.chdir(BASE_DIR)\n",
        "else:\n",
        "    BASE_DIR = Path.cwd()\n",
        "\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "OUTPUT_DIR = BASE_DIR / \"data\" / \"processed\"\n",
        "LMD_DIR = DATA_DIR / \"lmd_matched\"\n",
        "\n",
        "# Create directories\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(OUTPUT_DIR / \"abc\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"tokenized\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Working directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Model Classes\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention block.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        \n",
        "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
        "        self.c_proj = nn.Linear(d_model, d_model)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.d_model, dim=2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        y = F.scaled_dot_product_attention(q, k, v, dropout_p=0.1 if self.training else 0, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Feedforward network with GELU activation.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(d_model, d_ff)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: attention + feedforward with residual connections.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, d_ff, dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MusicTransformer(nn.Module):\n",
        "    \"\"\"Decoder-only Transformer for symbolic music generation.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model=512, n_layers=6, n_heads=8, \n",
        "                 d_ff=None, max_seq_length=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        if d_ff is None:\n",
        "            d_ff = 4 * d_model\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "        self.wte = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.wpe = nn.Embedding(max_seq_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_heads, d_ff, dropout) \n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        # Tie weights\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        tok_emb = self.wte(idx)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.wpe(pos)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), \n",
        "                targets.view(-1), \n",
        "                ignore_index=-1\n",
        "            )\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate new tokens given a context.\"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.max_seq_length:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "print(\"Model classes defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MIDI to ABC Converter\n",
        "class MIDIToABCConverter:\n",
        "    \"\"\"Convert MIDI files to ABC notation using music21.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.conversion_stats = {'success': 0, 'failed': 0, 'errors': []}\n",
        "    \n",
        "    def convert_midi_to_abc(self, midi_path: Path) -> Optional[str]:\n",
        "        \"\"\"Convert a MIDI file to ABC notation.\"\"\"\n",
        "        try:\n",
        "            null_stderr = io.StringIO()\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                with contextlib.redirect_stderr(null_stderr):\n",
        "                    score = music21.converter.parse(str(midi_path))\n",
        "            \n",
        "            abc_str = self._score_to_abc_manual(score)\n",
        "            \n",
        "            if abc_str:\n",
        "                abc_str = self._clean_abc(abc_str)\n",
        "                if len(abc_str.strip()) > 0:\n",
        "                    self.conversion_stats['success'] += 1\n",
        "                    return abc_str\n",
        "            \n",
        "            self.conversion_stats['failed'] += 1\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            self.conversion_stats['failed'] += 1\n",
        "            self.conversion_stats['errors'].append(str(e))\n",
        "            return None\n",
        "    \n",
        "    def _score_to_abc_manual(self, score) -> str:\n",
        "        \"\"\"Manually convert music21 score to ABC notation.\"\"\"\n",
        "        try:\n",
        "            abc_lines = []\n",
        "            \n",
        "            # ABC header\n",
        "            abc_lines.append(\"X:1\")\n",
        "            abc_lines.append(\"M:4/4\")  # Default time signature\n",
        "            abc_lines.append(\"L:1/8\")  # Default note length\n",
        "            abc_lines.append(\"K:C\")     # Default key\n",
        "            \n",
        "            # Extract time signature if available\n",
        "            for ts in score.flat.getElementsByClass('TimeSignature'):\n",
        "                if ts.numerator and ts.denominator:\n",
        "                    abc_lines[1] = f\"M:{ts.numerator}/{ts.denominator}\"\n",
        "                    break\n",
        "            \n",
        "            # Extract key signature if available\n",
        "            try:\n",
        "                key = score.analyze('key')\n",
        "                if key:\n",
        "                    key_name = key.tonic.name\n",
        "                    mode = 'maj' if key.mode == 'major' else 'min'\n",
        "                    abc_lines[3] = f\"K:{key_name}{mode[0]}\"\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # Convert notes to ABC body\n",
        "            abc_body = []\n",
        "            measure_count = 0\n",
        "            \n",
        "            for element in score.flat.notesAndRests:\n",
        "                if isinstance(element, music21.note.Note):\n",
        "                    abc_body.append(self._note_to_abc(element))\n",
        "                elif isinstance(element, music21.note.Rest):\n",
        "                    dur = self._duration_to_abc(element.duration.quarterLength)\n",
        "                    abc_body.append(\"z\" + dur)\n",
        "                elif isinstance(element, music21.chord.Chord):\n",
        "                    # Handle chords (simplified: use first note)\n",
        "                    if len(element.notes) > 0:\n",
        "                        abc_body.append(self._note_to_abc(element.notes[0]))\n",
        "                \n",
        "                # Add bar lines periodically\n",
        "                measure_count += 1\n",
        "                if measure_count % 4 == 0:\n",
        "                    abc_body.append(\"|\")\n",
        "            \n",
        "            body_str = \"\".join(abc_body)\n",
        "            if len(body_str) > 80:\n",
        "                parts = body_str.split(\"|\")\n",
        "                formatted_parts = []\n",
        "                for part in parts:\n",
        "                    if len(part) > 80:\n",
        "                        words = part.split()\n",
        "                        line = []\n",
        "                        for word in words:\n",
        "                            if len(\" \".join(line + [word])) > 80 and line:\n",
        "                                formatted_parts.append(\" \".join(line))\n",
        "                                line = [word]\n",
        "                            else:\n",
        "                                line.append(word)\n",
        "                        if line:\n",
        "                            formatted_parts.append(\" \".join(line))\n",
        "                    else:\n",
        "                        formatted_parts.append(part)\n",
        "                body_str = \"|\".join(formatted_parts)\n",
        "            \n",
        "            abc_lines.append(body_str)\n",
        "            return \"\\n\".join(abc_lines) if abc_lines else \"\"\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "    \n",
        "    def _note_to_abc(self, note) -> str:\n",
        "        \"\"\"Convert a music21 note to ABC notation.\"\"\"\n",
        "        try:\n",
        "            note_name = note.pitch.name[0]\n",
        "            \n",
        "            if note.pitch.accidental:\n",
        "                if note.pitch.accidental.alter == 1:\n",
        "                    note_name = \"^\" + note_name\n",
        "                elif note.pitch.accidental.alter == -1:\n",
        "                    note_name = \"_\" + note_name\n",
        "            \n",
        "            octave = note.pitch.octave\n",
        "            if octave < 4:\n",
        "                note_name = note_name.lower() * (4 - octave)\n",
        "            elif octave > 4:\n",
        "                note_name = note_name + \"'\" * (octave - 4)\n",
        "            \n",
        "            dur = self._duration_to_abc(note.duration.quarterLength)\n",
        "            return note_name + dur\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "    \n",
        "    def _duration_to_abc(self, quarter_length: float) -> str:\n",
        "        \"\"\"Convert duration in quarter notes to ABC notation.\"\"\"\n",
        "        eighth_notes = quarter_length * 2\n",
        "        eighth_notes = round(eighth_notes * 8) / 8\n",
        "        \n",
        "        if eighth_notes <= 0:\n",
        "            return \"\"\n",
        "        elif eighth_notes == 0.5:\n",
        "            return \"/\"\n",
        "        elif eighth_notes == 1.0:\n",
        "            return \"\"\n",
        "        elif eighth_notes == 2.0:\n",
        "            return \"2\"\n",
        "        elif eighth_notes == 3.0:\n",
        "            return \"3\"\n",
        "        elif eighth_notes == 4.0:\n",
        "            return \"4\"\n",
        "        elif eighth_notes == 6.0:\n",
        "            return \"6\"\n",
        "        elif eighth_notes == 8.0:\n",
        "            return \"8\"\n",
        "        else:\n",
        "            dur_int = int(eighth_notes)\n",
        "            if dur_int > 0 and dur_int <= 16:\n",
        "                return str(dur_int)\n",
        "            else:\n",
        "                return f\"/{int(1/eighth_notes)}\" if eighth_notes < 1 else str(int(eighth_notes))\n",
        "    \n",
        "    def _clean_abc(self, abc_str: str) -> str:\n",
        "        \"\"\"Clean and normalize ABC notation string.\"\"\"\n",
        "        lines = abc_str.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and not line.startswith('%'):\n",
        "                cleaned_lines.append(line)\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "print(\"MIDI to ABC converter defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Music Tokenizer\n",
        "class MusicTokenizer:\n",
        "    \"\"\"Tokenizer for ABC notation.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.vocab_size = 0\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "        self.special_tokens = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<START>': 2,\n",
        "            '<END>': 3,\n",
        "            '<SEP>': 4,\n",
        "        }\n",
        "    \n",
        "    def build_vocab(self, abc_strings: List[str], min_freq: int = 2):\n",
        "        \"\"\"Build vocabulary from ABC strings.\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        token_counter = Counter()\n",
        "        \n",
        "        for abc_str in tqdm(abc_strings, desc=\"Tokenizing for vocab\"):\n",
        "            tokens = self._tokenize_abc(abc_str)\n",
        "            token_counter.update(tokens)\n",
        "        \n",
        "        vocab = dict(self.special_tokens)\n",
        "        current_id = len(self.special_tokens)\n",
        "        \n",
        "        for token, count in token_counter.items():\n",
        "            if count >= min_freq:\n",
        "                vocab[token] = current_id\n",
        "                current_id += 1\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.token_to_id = vocab\n",
        "        self.id_to_token = {v: k for k, v in vocab.items()}\n",
        "        \n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"  Special tokens: {len(self.special_tokens)}\")\n",
        "        print(f\"  Regular tokens: {self.vocab_size - len(self.special_tokens)}\")\n",
        "    \n",
        "    def _tokenize_abc(self, abc_str: str) -> List[str]:\n",
        "        \"\"\"Tokenize ABC notation string into music-aware tokens.\"\"\"\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        \n",
        "        while i < len(abc_str):\n",
        "            char = abc_str[i]\n",
        "            \n",
        "            if char.isspace():\n",
        "                i += 1\n",
        "                continue\n",
        "            \n",
        "            if char == '|':\n",
        "                tokens.append('|')\n",
        "                i += 1\n",
        "                continue\n",
        "            \n",
        "            if char.upper() in 'ABCDEFG':\n",
        "                note_token = char.upper()\n",
        "                i += 1\n",
        "                \n",
        "                if i < len(abc_str) and abc_str[i] in '^_':\n",
        "                    note_token += abc_str[i]\n",
        "                    i += 1\n",
        "                \n",
        "                while i < len(abc_str) and abc_str[i] in \",'\":\n",
        "                    note_token += abc_str[i]\n",
        "                    i += 1\n",
        "                \n",
        "                tokens.append(note_token)\n",
        "                continue\n",
        "            \n",
        "            if char.isdigit():\n",
        "                duration = char\n",
        "                i += 1\n",
        "                while i < len(abc_str) and abc_str[i].isdigit():\n",
        "                    duration += abc_str[i]\n",
        "                    i += 1\n",
        "                tokens.append(f\"DUR:{duration}\")\n",
        "                continue\n",
        "            \n",
        "            if char == 'z':\n",
        "                tokens.append('z')\n",
        "                i += 1\n",
        "                continue\n",
        "            \n",
        "            tokens.append(char)\n",
        "            i += 1\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def encode(self, abc_str: str) -> List[int]:\n",
        "        \"\"\"Encode ABC string to token IDs.\"\"\"\n",
        "        tokens = self._tokenize_abc(abc_str)\n",
        "        token_ids = []\n",
        "        for token in tokens:\n",
        "            if token in self.token_to_id:\n",
        "                token_ids.append(self.token_to_id[token])\n",
        "            else:\n",
        "                token_ids.append(self.token_to_id['<UNK>'])\n",
        "        return token_ids\n",
        "    \n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs back to ABC string.\"\"\"\n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id in self.id_to_token:\n",
        "                tokens.append(self.id_to_token[token_id])\n",
        "            else:\n",
        "                tokens.append('<UNK>')\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    def save(self, path: Path):\n",
        "        \"\"\"Save tokenizer to disk.\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'vocab': self.vocab,\n",
        "                'token_to_id': self.token_to_id,\n",
        "                'id_to_token': self.id_to_token,\n",
        "                'vocab_size': self.vocab_size,\n",
        "                'special_tokens': self.special_tokens\n",
        "            }, f)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, path: Path):\n",
        "        \"\"\"Load tokenizer from disk.\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        tokenizer = cls()\n",
        "        tokenizer.vocab = data['vocab']\n",
        "        tokenizer.token_to_id = data['token_to_id']\n",
        "        tokenizer.id_to_token = data['id_to_token']\n",
        "        tokenizer.vocab_size = data['vocab_size']\n",
        "        tokenizer.special_tokens = data['special_tokens']\n",
        "        return tokenizer\n",
        "\n",
        "print(\"Music tokenizer defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Collection\n",
        "\n",
        "This section downloads and extracts the Lakh MIDI Dataset (LMD-matched) to the `/data` folder.\n",
        "\n",
        "**Note**: The dataset is ~1.7GB compressed and ~2-3GB extracted. This may take several minutes to download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Collection: Download and Extract Lakh MIDI Dataset\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "LMD_URL = \"http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz\"\n",
        "TAR_PATH = DATA_DIR / \"lmd_matched.tar.gz\"\n",
        "\n",
        "def download_with_progress(url: str, destination: Path):\n",
        "    \"\"\"Download file with progress bar.\"\"\"\n",
        "    def reporthook(count, block_size, total_size):\n",
        "        percent = int(count * block_size * 100 / total_size)\n",
        "        print(f\"\\rDownloading... {percent}% ({count * block_size / 1024 / 1024:.1f} MB / {total_size / 1024 / 1024:.1f} MB)\", end='', flush=True)\n",
        "    \n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, destination, reporthook=reporthook)\n",
        "        print(\"\\n✓ Download complete!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check if dataset already exists\n",
        "if LMD_DIR.exists() and len(list(LMD_DIR.rglob(\"*.mid\"))) > 0:\n",
        "    print(f\"✓ LMD dataset already exists in {LMD_DIR}\")\n",
        "    print(f\"  Found {len(list(LMD_DIR.rglob('*.mid')))} MIDI files\")\n",
        "else:\n",
        "    print(\"LMD dataset not found. Starting download...\")\n",
        "    print(f\"URL: {LMD_URL}\")\n",
        "    print(f\"Destination: {TAR_PATH}\")\n",
        "    print(f\"Note: This is a large file (~1.7GB), download may take 5-15 minutes depending on connection speed.\\n\")\n",
        "    \n",
        "    # Download the dataset\n",
        "    if not TAR_PATH.exists():\n",
        "        if download_with_progress(LMD_URL, TAR_PATH):\n",
        "            print(f\"✓ Downloaded to {TAR_PATH}\")\n",
        "        else:\n",
        "            print(\"✗ Download failed. Please check your internet connection and try again.\")\n",
        "            raise RuntimeError(\"Failed to download LMD dataset\")\n",
        "    else:\n",
        "        print(f\"✓ Tar file already exists: {TAR_PATH}\")\n",
        "    \n",
        "    # Extract the dataset\n",
        "    print(f\"\\nExtracting {TAR_PATH} to {DATA_DIR}...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "    \n",
        "    try:\n",
        "        with tarfile.open(TAR_PATH, 'r:gz') as tar:\n",
        "            # Get total members for progress\n",
        "            members = tar.getmembers()\n",
        "            total = len(members)\n",
        "            \n",
        "            # Extract with progress\n",
        "            for i, member in enumerate(members):\n",
        "                tar.extract(member, DATA_DIR)\n",
        "                if (i + 1) % 1000 == 0:\n",
        "                    print(f\"  Extracted {i+1}/{total} files...\", end='\\r', flush=True)\n",
        "            \n",
        "            print(f\"\\n✓ Extracted {total} files\")\n",
        "        \n",
        "        # The tar file might extract to a folder with a different name\n",
        "        # Check for extracted folders and rename if needed\n",
        "        extracted_folders = [d for d in DATA_DIR.iterdir() \n",
        "                           if d.is_dir() and 'lmd' in d.name.lower() and d != LMD_DIR]\n",
        "        \n",
        "        if extracted_folders and not LMD_DIR.exists():\n",
        "            if len(extracted_folders) == 1:\n",
        "                print(f\"Renaming {extracted_folders[0]} to {LMD_DIR}\")\n",
        "                extracted_folders[0].rename(LMD_DIR)\n",
        "        \n",
        "        # Clean up tar file to save space (optional - comment out if you want to keep it)\n",
        "        # TAR_PATH.unlink()\n",
        "        # print(f\"✓ Removed tar file to save space\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Extraction failed: {e}\")\n",
        "        raise RuntimeError(f\"Failed to extract dataset: {e}\")\n",
        "\n",
        "# Verify MIDI files are available\n",
        "midi_files = list(LMD_DIR.rglob(\"*.mid\"))\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Data Collection Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"MIDI files found: {len(midi_files):,}\")\n",
        "print(f\"Location: {LMD_DIR}\")\n",
        "\n",
        "if len(midi_files) == 0:\n",
        "    print(\"\\n⚠ WARNING: No MIDI files found!\")\n",
        "    print(\"Please check:\")\n",
        "    print(f\"  1. Extraction completed successfully\")\n",
        "    print(f\"  2. Files are in: {LMD_DIR}\")\n",
        "    print(f\"  3. Directory structure is correct\")\n",
        "    raise RuntimeError(\"No MIDI files found after download/extraction\")\n",
        "\n",
        "# Limit to 10,000 files for this run\n",
        "MAX_FILES = 10000\n",
        "if len(midi_files) > MAX_FILES:\n",
        "    print(f\"\\nLimiting to {MAX_FILES} files for processing\")\n",
        "    midi_files = midi_files[:MAX_FILES]\n",
        "\n",
        "print(f\"\\n✓ Ready to process {len(midi_files):,} MIDI files\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Processing: Convert MIDI to ABC Notation\n",
        "\n",
        "This section processes the downloaded MIDI files and converts them to ABC notation format.\n",
        "\n",
        "**Processing**: Up to 10,000 MIDI files will be converted to ABC notation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert MIDI files to ABC notation\n",
        "converter = MIDIToABCConverter()\n",
        "abc_data = []\n",
        "\n",
        "print(f\"Converting {len(midi_files)} MIDI files to ABC notation...\")\n",
        "print(\"This may take a while. Progress will be shown below.\")\n",
        "\n",
        "for midi_file in tqdm(midi_files, desc=\"Converting MIDI to ABC\"):\n",
        "    abc_str = converter.convert_midi_to_abc(midi_file)\n",
        "    \n",
        "    if abc_str:\n",
        "        # Save ABC file\n",
        "        abc_path = OUTPUT_DIR / \"abc\" / f\"{midi_file.stem}.abc\"\n",
        "        abc_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        with open(abc_path, 'w') as f:\n",
        "            f.write(abc_str)\n",
        "        \n",
        "        abc_data.append((midi_file, abc_str))\n",
        "    \n",
        "    # Periodic garbage collection\n",
        "    if len(abc_data) % 100 == 0:\n",
        "        gc.collect()\n",
        "\n",
        "print(f\"\\nConversion complete!\")\n",
        "print(f\"  Successful: {converter.conversion_stats['success']}\")\n",
        "print(f\"  Failed: {converter.conversion_stats['failed']}\")\n",
        "print(f\"  Success rate: {converter.conversion_stats['success'] / len(midi_files) * 100:.1f}%\")\n",
        "print(f\"  Total ABC files: {len(abc_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Vocabulary and Tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vocabulary from ABC strings\n",
        "print(\"Building vocabulary...\")\n",
        "tokenizer = MusicTokenizer()\n",
        "abc_strings = [abc_str for _, abc_str in abc_data]\n",
        "tokenizer.build_vocab(abc_strings, min_freq=2)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save(OUTPUT_DIR / \"tokenizer.pkl\")\n",
        "print(f\"Tokenizer saved to {OUTPUT_DIR / 'tokenizer.pkl'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Filter Sequences and Create Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter sequences by length and tokenize\n",
        "MIN_SEQUENCE_LENGTH = 50\n",
        "MAX_SEQUENCE_LENGTH = 5000\n",
        "\n",
        "print(f\"Filtering sequences (length: {MIN_SEQUENCE_LENGTH}-{MAX_SEQUENCE_LENGTH} tokens)...\")\n",
        "\n",
        "filtered_data = []\n",
        "stats = {'too_short': 0, 'too_long': 0, 'valid': 0}\n",
        "\n",
        "for midi_path, abc_str in tqdm(abc_data, desc=\"Filtering and tokenizing\"):\n",
        "    token_ids = tokenizer.encode(abc_str)\n",
        "    seq_length = len(token_ids)\n",
        "    \n",
        "    if seq_length < MIN_SEQUENCE_LENGTH:\n",
        "        stats['too_short'] += 1\n",
        "        continue\n",
        "    elif seq_length > MAX_SEQUENCE_LENGTH:\n",
        "        stats['too_long'] += 1\n",
        "        continue\n",
        "    else:\n",
        "        stats['valid'] += 1\n",
        "        filtered_data.append((midi_path, abc_str, token_ids))\n",
        "\n",
        "print(f\"\\nFiltering statistics:\")\n",
        "print(f\"  Too short (<{MIN_SEQUENCE_LENGTH}): {stats['too_short']}\")\n",
        "print(f\"  Too long (>{MAX_SEQUENCE_LENGTH}): {stats['too_long']}\")\n",
        "print(f\"  Valid: {stats['valid']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/val/test splits\n",
        "TRAIN_SPLIT = 0.98\n",
        "VAL_SPLIT = 0.01\n",
        "TEST_SPLIT = 0.01\n",
        "\n",
        "print(\"Creating train/val/test splits...\")\n",
        "\n",
        "# Shuffle data\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(filtered_data))\n",
        "filtered_data = [filtered_data[i] for i in indices]\n",
        "\n",
        "# Calculate split indices\n",
        "n_total = len(filtered_data)\n",
        "n_train = int(n_total * TRAIN_SPLIT)\n",
        "n_val = int(n_total * VAL_SPLIT)\n",
        "\n",
        "train_data = filtered_data[:n_train]\n",
        "val_data = filtered_data[n_train:n_train + n_val]\n",
        "test_data = filtered_data[n_train + n_val:]\n",
        "\n",
        "print(f\"  Train: {len(train_data)} sequences\")\n",
        "print(f\"  Val: {len(val_data)} sequences\")\n",
        "print(f\"  Test: {len(test_data)} sequences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save splits to disk\n",
        "print(\"Saving splits...\")\n",
        "\n",
        "def save_split(data, split_name):\n",
        "    split_dir = OUTPUT_DIR / \"tokenized\" / split_name\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    json_data = []\n",
        "    for midi_path, abc_str, token_ids in data:\n",
        "        json_data.append({\n",
        "            'midi_path': str(midi_path),\n",
        "            'abc': abc_str,\n",
        "            'tokens': token_ids,\n",
        "            'length': len(token_ids)\n",
        "        })\n",
        "    \n",
        "    with open(split_dir / \"data.json\", 'w') as f:\n",
        "        json.dump(json_data, f, indent=2)\n",
        "\n",
        "save_split(train_data, \"train\")\n",
        "save_split(val_data, \"val\")\n",
        "save_split(test_data, \"test\")\n",
        "\n",
        "# Calculate statistics\n",
        "train_tokens = sum(len(tokens) for _, _, tokens in train_data)\n",
        "val_tokens = sum(len(tokens) for _, _, tokens in val_data)\n",
        "test_tokens = sum(len(tokens) for _, _, tokens in test_data)\n",
        "\n",
        "print(f\"\\nToken counts:\")\n",
        "print(f\"  Train: {train_tokens:,} tokens ({train_tokens/1e6:.1f}M)\")\n",
        "print(f\"  Val: {val_tokens:,} tokens\")\n",
        "print(f\"  Test: {test_tokens:,} tokens\")\n",
        "print(f\"  Total: {train_tokens + val_tokens + test_tokens:,} tokens\")\n",
        "\n",
        "print(f\"\\n✓ Data preprocessing complete!\")\n",
        "print(f\"  Tokenizer: {OUTPUT_DIR / 'tokenizer.pkl'}\")\n",
        "print(f\"  Train data: {OUTPUT_DIR / 'tokenized' / 'train' / 'data.json'}\")\n",
        "print(f\"  Val data: {OUTPUT_DIR / 'tokenized' / 'val' / 'data.json'}\")\n",
        "print(f\"  Test data: {OUTPUT_DIR / 'tokenized' / 'test' / 'data.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Loading Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading classes\n",
        "class MusicDataset(Dataset):\n",
        "    \"\"\"Dataset for tokenized music sequences.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path: Path, max_seq_length: int = 5000):\n",
        "        self.data_path = data_path\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.sequences = []\n",
        "        \n",
        "        print(f\"Loading sequences from {data_path}...\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            try:\n",
        "                data = json.loads(content)\n",
        "                if isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        token_ids = item.get('token_ids') or item.get('tokens', [])\n",
        "                        if len(token_ids) > max_seq_length:\n",
        "                            token_ids = token_ids[:max_seq_length]\n",
        "                        if len(token_ids) > 0:\n",
        "                            self.sequences.append(token_ids)\n",
        "            except json.JSONDecodeError:\n",
        "                f.seek(0)\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        data = json.loads(line)\n",
        "                        token_ids = data.get('token_ids') or data.get('tokens', [])\n",
        "                        if len(token_ids) > max_seq_length:\n",
        "                            token_ids = token_ids[:max_seq_length]\n",
        "                        if len(token_ids) > 0:\n",
        "                            self.sequences.append(token_ids)\n",
        "        \n",
        "        print(f\"Loaded {len(self.sequences)} sequences\")\n",
        "        if len(self.sequences) > 0:\n",
        "            avg_len = sum(len(s) for s in self.sequences) / len(self.sequences)\n",
        "            print(f\"Average sequence length: {avg_len:.1f}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        tokens = torch.tensor(sequence, dtype=torch.long)\n",
        "        input_ids = tokens[:-1]\n",
        "        target_ids = tokens[1:]\n",
        "        return input_ids, target_ids\n",
        "\n",
        "\n",
        "class MusicDataLoader:\n",
        "    \"\"\"Data loader that batches by tokens (not sequences).\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path: Path, batch_size_tokens: int, \n",
        "                 max_seq_length: int = 5000, shuffle: bool = True):\n",
        "        self.data_path = data_path\n",
        "        self.batch_size_tokens = batch_size_tokens\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        self.dataset = MusicDataset(data_path, max_seq_length)\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=0,\n",
        "            collate_fn=lambda batch: batch[0]\n",
        "        )\n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\"Create batches based on token count.\"\"\"\n",
        "        batch_inputs = []\n",
        "        batch_targets = []\n",
        "        current_batch_tokens = 0\n",
        "        \n",
        "        for input_ids, target_ids in self.dataloader:\n",
        "            seq_len = input_ids.size(0)\n",
        "            \n",
        "            if current_batch_tokens + seq_len > self.batch_size_tokens and len(batch_inputs) > 0:\n",
        "                max_len = max(seq.size(0) for seq in batch_inputs)\n",
        "                padded_inputs = []\n",
        "                padded_targets = []\n",
        "                \n",
        "                for inp, tgt in zip(batch_inputs, batch_targets):\n",
        "                    pad_len = max_len - inp.size(0)\n",
        "                    if pad_len > 0:\n",
        "                        inp = torch.cat([inp, torch.full((pad_len,), -1, dtype=inp.dtype)])\n",
        "                        tgt = torch.cat([tgt, torch.full((pad_len,), -1, dtype=tgt.dtype)])\n",
        "                    padded_inputs.append(inp)\n",
        "                    padded_targets.append(tgt)\n",
        "                \n",
        "                yield torch.stack(padded_inputs), torch.stack(padded_targets)\n",
        "                \n",
        "                batch_inputs = []\n",
        "                batch_targets = []\n",
        "                current_batch_tokens = 0\n",
        "            \n",
        "            batch_inputs.append(input_ids)\n",
        "            batch_targets.append(target_ids)\n",
        "            current_batch_tokens += seq_len\n",
        "        \n",
        "        if len(batch_inputs) > 0:\n",
        "            max_len = max(seq.size(0) for seq in batch_inputs)\n",
        "            padded_inputs = []\n",
        "            padded_targets = []\n",
        "            \n",
        "            for inp, tgt in zip(batch_inputs, batch_targets):\n",
        "                pad_len = max_len - inp.size(0)\n",
        "                if pad_len > 0:\n",
        "                    inp = torch.cat([inp, torch.full((pad_len,), -1, dtype=inp.dtype)])\n",
        "                    tgt = torch.cat([tgt, torch.full((pad_len,), -1, dtype=tgt.dtype)])\n",
        "                padded_inputs.append(inp)\n",
        "                padded_targets.append(tgt)\n",
        "            \n",
        "            yield torch.stack(padded_inputs), torch.stack(padded_targets)\n",
        "\n",
        "print(\"Data loading utilities defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def get_lr_schedule(optimizer, num_steps, warmup_steps=0):\n",
        "    \"\"\"Create cosine annealing learning rate schedule.\"\"\"\n",
        "    if warmup_steps > 0:\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (num_steps - warmup_steps)\n",
        "                return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    else:\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, scheduler, device, log_interval=100):\n",
        "    \"\"\"Train model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_steps = 0\n",
        "    \n",
        "    for step, (input_ids, target_ids) in enumerate(train_loader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "        \n",
        "        # Clamp token IDs to valid range\n",
        "        vocab_size = model.vocab_size\n",
        "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
        "        target_ids = torch.clamp(target_ids, 0, vocab_size - 1)\n",
        "        input_ids = torch.where(input_ids == -1, torch.tensor(0, device=device), input_ids)\n",
        "        \n",
        "        # Forward pass\n",
        "        logits, loss = model(input_ids, target_ids)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_steps += 1\n",
        "        \n",
        "        if step % log_interval == 0:\n",
        "            avg_loss = total_loss / num_steps\n",
        "            current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']\n",
        "            print(f\"Step {step:6d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
        "    \n",
        "    avg_loss = total_loss / num_steps\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_steps = 0\n",
        "    \n",
        "    for input_ids, target_ids in val_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "        \n",
        "        vocab_size = model.vocab_size\n",
        "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
        "        target_ids = torch.clamp(target_ids, 0, vocab_size - 1)\n",
        "        input_ids = torch.where(input_ids == -1, torch.tensor(0, device=device), input_ids)\n",
        "        \n",
        "        logits, loss = model(input_ids, target_ids)\n",
        "        total_loss += loss.item()\n",
        "        num_steps += 1\n",
        "    \n",
        "    avg_loss = total_loss / num_steps if num_steps > 0 else float('inf')\n",
        "    return avg_loss\n",
        "\n",
        "print(\"Training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = MusicTokenizer.load(OUTPUT_DIR / \"tokenizer.pkl\")\n",
        "print(f\"Tokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model configuration (Tiny model for quick testing)\n",
        "# You can modify these for different model sizes\n",
        "MODEL_CONFIG = {\n",
        "    'd_model': 128,\n",
        "    'n_layers': 2,\n",
        "    'n_heads': 2,\n",
        "    'd_ff': 512,\n",
        "    'max_seq_length': 5000,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "model = MusicTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    **MODEL_CONFIG\n",
        ").to(device)\n",
        "\n",
        "num_params = model.count_parameters()\n",
        "print(f\"\\nModel initialized:\")\n",
        "print(f\"  Parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "print(f\"  Config: {MODEL_CONFIG}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_path = OUTPUT_DIR / \"tokenized\" / \"train\" / \"data.json\"\n",
        "val_path = OUTPUT_DIR / \"tokenized\" / \"val\" / \"data.json\"\n",
        "\n",
        "BATCH_SIZE_TOKENS = 50000  # Target tokens per batch\n",
        "MAX_SEQ_LENGTH = MODEL_CONFIG['max_seq_length']\n",
        "\n",
        "train_loader = MusicDataLoader(\n",
        "    train_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = MusicDataLoader(\n",
        "    val_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Data loaders created:\")\n",
        "print(f\"  Batch size (tokens): {BATCH_SIZE_TOKENS:,}\")\n",
        "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_EPOCHS = 1  # For scaling laws study, typically train for 1 epoch\n",
        "WARMUP_STEPS = 0\n",
        "LOG_INTERVAL = 100\n",
        "\n",
        "# Estimate number of steps (approximate)\n",
        "# You can adjust this based on your actual data size\n",
        "estimated_steps = len(train_loader) if hasattr(train_loader, '__len__') else 1000\n",
        "\n",
        "# Setup optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = get_lr_schedule(optimizer, estimated_steps, warmup_steps=WARMUP_STEPS)\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Estimated steps per epoch: {estimated_steps}\")\n",
        "print(f\"  Log interval: {LOG_INTERVAL}\")\n",
        "\n",
        "# Train for 1 epoch\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training for {NUM_EPOCHS} epoch(s)...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    train_loss = train_one_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device,\n",
        "        log_interval=LOG_INTERVAL\n",
        "    )\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training complete!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Model and Results\n",
        "\n",
        "You can save the trained model and use it for generation or further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_save_path = OUTPUT_DIR / \"model.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': MODEL_CONFIG,\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_parameters': num_params,\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "# Optional: Test generation\n",
        "print(\"\\nTesting generation...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Create a simple starting sequence (just a few tokens)\n",
        "    start_tokens = torch.tensor([[tokenizer.token_to_id.get('C', 0)]], device=device)\n",
        "    generated = model.generate(start_tokens, max_new_tokens=50, temperature=1.0)\n",
        "    \n",
        "    # Decode generated tokens\n",
        "    generated_tokens = generated[0].cpu().tolist()\n",
        "    generated_abc = tokenizer.decode(generated_tokens)\n",
        "    print(f\"Generated ABC (first 50 tokens): {generated_abc[:200]}...\")\n",
        "\n",
        "print(\"\\n✓ All done! You can now:\")\n",
        "print(\"  1. Modify MODEL_CONFIG to train different model sizes\")\n",
        "print(\"  2. Train multiple models and collect validation losses\")\n",
        "print(\"  3. Plot scaling laws: validation loss vs. model size\")\n",
        "print(\"  4. Generate music samples from trained models\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
