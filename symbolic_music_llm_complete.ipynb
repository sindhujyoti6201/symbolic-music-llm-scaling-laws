{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYxLvtmw9JnJ"
      },
      "source": [
        "# Symbolic Music LLM Scaling Laws - Complete Pipeline\n",
        "\n",
        "This notebook implements the complete pipeline for the CS-GY 6923 project on scaling laws for symbolic music language models.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook covers all required components:\n",
        "1. **Setup and Configuration** - Install dependencies and configure paths\n",
        "2. **Data Collection** - Download and extract Lakh MIDI Dataset\n",
        "3. **Data Preprocessing** - Convert MIDI files to ABC notation using `midi_to_abc.py`\n",
        "4. **Tokenization** - Build vocabulary and tokenize sequences using `tokenizer.py`\n",
        "5. **Data Splitting** - Create train/validation/test splits using `process_pipeline.py`\n",
        "6. **Model Architecture** - Define Transformer and RNN models\n",
        "7. **Training Utilities** - Training functions from `train.py` and `utils/training.py`\n",
        "8. **Transformer Scaling Study** - Train 5 model sizes using `scaling_study.py`\n",
        "9. **RNN Scaling Study** - Train RNN models for comparison\n",
        "10. **Scaling Analysis** - Plot scaling laws using `plot_scaling.py`\n",
        "11. **Sample Generation** - Generate and evaluate music samples using `sample_generation.py`\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "1. **Enable GPU**: Runtime → Change runtime type → GPU (recommended)\n",
        "2. **Run cells sequentially**: Execute cells from top to bottom\n",
        "3. **Expected runtime**:\n",
        "   - Data collection: 5-15 minutes (download)\n",
        "   - Data preprocessing: 5-10 minutes (1K files)\n",
        "   - Training: Varies by model size (1-5 hours total)\n",
        "\n",
        "## Output Files\n",
        "\n",
        "All outputs are saved to `data/processed/`:\n",
        "- ABC files: `data/processed/abc/`\n",
        "- Tokenized data: `data/processed/tokenized/`\n",
        "- Models: `data/processed/`\n",
        "- Samples: `data/processed/samples/`\n",
        "- Results: `data/processed/scaling_results.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fv5MmYH09JnL",
        "outputId": "fc093b22-3bf6-457b-a22f-1a7c0826067d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m5.5/5.6 MB\u001b[0m \u001b[31m164.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q music21 pretty_midi librosa mir_eval numpy pandas scipy torch transformers datasets matplotlib seaborn tqdm pyyaml wandb joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NsBWdEd9JnL"
      },
      "source": [
        "# Part 1: Setup and Configuration\n",
        "\n",
        "This section sets up the environment, installs dependencies, and configures paths for the project.\n",
        "\n",
        "## 1.1 Install Dependencies\n",
        "\n",
        "Install all required packages for music processing, deep learning, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0wCxip79JnL",
        "outputId": "6c3878d2-bfaf-44e7-ab2f-f7db08c2a753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /Users/sd6201/Documents/Workspace/symbolic-music-llm-scaling-laws\n",
            "Data directory: /Users/sd6201/Documents/Workspace/symbolic-music-llm-scaling-laws/data\n",
            "Output directory: /Users/sd6201/Documents/Workspace/symbolic-music-llm-scaling-laws/data/processed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import random\n",
        "import gc\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "# Aggressively suppress all warnings, especially from music21\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "import music21\n",
        "music21.environment.UserSettings()['warnings'] = 0\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Setup paths for Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    BASE_DIR = Path('/content/symbolic-music-llm')\n",
        "    BASE_DIR.mkdir(exist_ok=True)\n",
        "    os.chdir(BASE_DIR)\n",
        "else:\n",
        "    BASE_DIR = Path.cwd()\n",
        "\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "OUTPUT_DIR = BASE_DIR / \"data\" / \"processed\"\n",
        "LMD_DIR = DATA_DIR / \"lmd_matched\"\n",
        "\n",
        "# Create directories\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(OUTPUT_DIR / \"abc\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"tokenized\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Working directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0OmjP789JnM"
      },
      "source": [
        "# Part 2: Model Architecture\n",
        "\n",
        "This section defines the neural network architectures used for symbolic music generation. The code is based on `models/transformer.py` and includes both Transformer and RNN/LSTM models.\n",
        "\n",
        "## 2.1 Transformer Architecture\n",
        "\n",
        "The Transformer model uses a decoder-only architecture (similar to GPT) with:\n",
        "- Multi-head causal self-attention\n",
        "- Feedforward networks with GELU activation\n",
        "- Layer normalization and residual connections\n",
        "- Positional embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6G4lx5f9JnM",
        "outputId": "58a17029-18e3-4014-ffb8-fd8fddcca89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model classes defined (Transformer + LSTM)!\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model Classes\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention block.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
        "        self.c_proj = nn.Linear(d_model, d_model)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.d_model, dim=2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, dropout_p=0.1 if self.training else 0, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Feedforward network with GELU activation.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(d_model, d_ff)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: attention + feedforward with residual connections.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MusicTransformer(nn.Module):\n",
        "    \"\"\"Decoder-only Transformer for symbolic music generation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=512, n_layers=6, n_heads=8,\n",
        "                 d_ff=None, max_seq_length=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        if d_ff is None:\n",
        "            d_ff = 4 * d_model\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.wte = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.wpe = nn.Embedding(max_seq_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Tie weights\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        tok_emb = self.wte(idx)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.wpe(pos)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate new tokens given a context.\"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.max_seq_length:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# RNN/LSTM Model Classes\n",
        "class MusicLSTM(nn.Module):\n",
        "    \"\"\"LSTM-based model for symbolic music generation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=512, n_layers=2, dropout=0.1, max_seq_length=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            d_model,\n",
        "            d_model,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "\n",
        "        # Embedding\n",
        "        x = self.embedding(idx)  # (B, T, d_model)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(x)  # (B, T, d_model)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Output projection\n",
        "        logits = self.lm_head(lstm_out)  # (B, T, vocab_size)\n",
        "\n",
        "        # Compute loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate new tokens given a context.\"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if needed\n",
        "            idx_cond = idx[:, -self.max_seq_length:] if idx.size(1) > self.max_seq_length else idx\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "print(\"Model classes defined (Transformer + LSTM)!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk_t6Lia9JnM"
      },
      "source": [
        "# Part 3: Data Processing Utilities\n",
        "\n",
        "This section includes utilities for converting MIDI to ABC notation and tokenizing music sequences. The code is based on `data-collection-and-preprocessing/midi_to_abc.py` and `data-collection-and-preprocessing/tokenizer.py`.\n",
        "\n",
        "## 3.1 MIDI to ABC Converter\n",
        "\n",
        "The `MIDIToABCConverter` class converts MIDI files to ABC notation using music21. This handles:\n",
        "- Parsing MIDI files\n",
        "- Extracting musical elements (notes, rests, chords)\n",
        "- Converting to ABC format with proper headers\n",
        "- Cleaning and normalizing output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snn-mNLo9JnM",
        "outputId": "8ef49174-4eb4-4373-c003-eaf6c20254f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIDI to ABC converter defined!\n"
          ]
        }
      ],
      "source": [
        "# MIDI to ABC Converter\n",
        "class MIDIToABCConverter:\n",
        "    \"\"\"Convert MIDI files to ABC notation using music21.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conversion_stats = {'success': 0, 'failed': 0, 'errors': []}\n",
        "\n",
        "    def convert_midi_to_abc(self, midi_path: Path) -> Optional[str]:\n",
        "        \"\"\"Convert a MIDI file to ABC notation.\"\"\"\n",
        "        try:\n",
        "            null_stderr = io.StringIO()\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                with contextlib.redirect_stderr(null_stderr):\n",
        "                    score = music21.converter.parse(str(midi_path))\n",
        "\n",
        "            abc_str = self._score_to_abc_manual(score)\n",
        "\n",
        "            if abc_str:\n",
        "                abc_str = self._clean_abc(abc_str)\n",
        "                if len(abc_str.strip()) > 0:\n",
        "                    self.conversion_stats['success'] += 1\n",
        "                    return abc_str\n",
        "\n",
        "            self.conversion_stats['failed'] += 1\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            self.conversion_stats['failed'] += 1\n",
        "            self.conversion_stats['errors'].append(str(e))\n",
        "            return None\n",
        "\n",
        "    def _score_to_abc_manual(self, score) -> str:\n",
        "        \"\"\"Manually convert music21 score to ABC notation.\"\"\"\n",
        "        try:\n",
        "            abc_lines = []\n",
        "\n",
        "            # ABC header\n",
        "            abc_lines.append(\"X:1\")\n",
        "            abc_lines.append(\"M:4/4\")  # Default time signature\n",
        "            abc_lines.append(\"L:1/8\")  # Default note length\n",
        "            abc_lines.append(\"K:C\")     # Default key\n",
        "\n",
        "            # Extract time signature if available\n",
        "            for ts in score.flat.getElementsByClass('TimeSignature'):\n",
        "                if ts.numerator and ts.denominator:\n",
        "                    abc_lines[1] = f\"M:{ts.numerator}/{ts.denominator}\"\n",
        "                    break\n",
        "\n",
        "            # Extract key signature if available\n",
        "            try:\n",
        "                key = score.analyze('key')\n",
        "                if key:\n",
        "                    key_name = key.tonic.name\n",
        "                    mode = 'maj' if key.mode == 'major' else 'min'\n",
        "                    abc_lines[3] = f\"K:{key_name}{mode[0]}\"\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Convert notes to ABC body\n",
        "            abc_body = []\n",
        "            measure_count = 0\n",
        "\n",
        "            for element in score.flat.notesAndRests:\n",
        "                if isinstance(element, music21.note.Note):\n",
        "                    abc_body.append(self._note_to_abc(element))\n",
        "                elif isinstance(element, music21.note.Rest):\n",
        "                    dur = self._duration_to_abc(element.duration.quarterLength)\n",
        "                    abc_body.append(\"z\" + dur)\n",
        "                elif isinstance(element, music21.chord.Chord):\n",
        "                    # Handle chords (simplified: use first note)\n",
        "                    if len(element.notes) > 0:\n",
        "                        abc_body.append(self._note_to_abc(element.notes[0]))\n",
        "\n",
        "                # Add bar lines periodically\n",
        "                measure_count += 1\n",
        "                if measure_count % 4 == 0:\n",
        "                    abc_body.append(\"|\")\n",
        "\n",
        "            body_str = \"\".join(abc_body)\n",
        "            if len(body_str) > 80:\n",
        "                parts = body_str.split(\"|\")\n",
        "                formatted_parts = []\n",
        "                for part in parts:\n",
        "                    if len(part) > 80:\n",
        "                        words = part.split()\n",
        "                        line = []\n",
        "                        for word in words:\n",
        "                            if len(\" \".join(line + [word])) > 80 and line:\n",
        "                                formatted_parts.append(\" \".join(line))\n",
        "                                line = [word]\n",
        "                            else:\n",
        "                                line.append(word)\n",
        "                        if line:\n",
        "                            formatted_parts.append(\" \".join(line))\n",
        "                    else:\n",
        "                        formatted_parts.append(part)\n",
        "                body_str = \"|\".join(formatted_parts)\n",
        "\n",
        "            abc_lines.append(body_str)\n",
        "            return \"\\n\".join(abc_lines) if abc_lines else \"\"\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "\n",
        "    def _note_to_abc(self, note) -> str:\n",
        "        \"\"\"Convert a music21 note to ABC notation.\"\"\"\n",
        "        try:\n",
        "            note_name = note.pitch.name[0]\n",
        "\n",
        "            if note.pitch.accidental:\n",
        "                if note.pitch.accidental.alter == 1:\n",
        "                    note_name = \"^\" + note_name\n",
        "                elif note.pitch.accidental.alter == -1:\n",
        "                    note_name = \"_\" + note_name\n",
        "\n",
        "            octave = note.pitch.octave\n",
        "            if octave < 4:\n",
        "                note_name = note_name.lower() * (4 - octave)\n",
        "            elif octave > 4:\n",
        "                note_name = note_name + \"'\" * (octave - 4)\n",
        "\n",
        "            dur = self._duration_to_abc(note.duration.quarterLength)\n",
        "            return note_name + dur\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def _duration_to_abc(self, quarter_length: float) -> str:\n",
        "        \"\"\"Convert duration in quarter notes to ABC notation.\"\"\"\n",
        "        eighth_notes = quarter_length * 2\n",
        "        eighth_notes = round(eighth_notes * 8) / 8\n",
        "\n",
        "        if eighth_notes <= 0:\n",
        "            return \"\"\n",
        "        elif eighth_notes == 0.5:\n",
        "            return \"/\"\n",
        "        elif eighth_notes == 1.0:\n",
        "            return \"\"\n",
        "        elif eighth_notes == 2.0:\n",
        "            return \"2\"\n",
        "        elif eighth_notes == 3.0:\n",
        "            return \"3\"\n",
        "        elif eighth_notes == 4.0:\n",
        "            return \"4\"\n",
        "        elif eighth_notes == 6.0:\n",
        "            return \"6\"\n",
        "        elif eighth_notes == 8.0:\n",
        "            return \"8\"\n",
        "        else:\n",
        "            dur_int = int(eighth_notes)\n",
        "            if dur_int > 0 and dur_int <= 16:\n",
        "                return str(dur_int)\n",
        "            else:\n",
        "                return f\"/{int(1/eighth_notes)}\" if eighth_notes < 1 else str(int(eighth_notes))\n",
        "\n",
        "    def _clean_abc(self, abc_str: str) -> str:\n",
        "        \"\"\"Clean and normalize ABC notation string.\"\"\"\n",
        "        lines = abc_str.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and not line.startswith('%'):\n",
        "                cleaned_lines.append(line)\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "print(\"MIDI to ABC converter defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paSUcmrt9JnN"
      },
      "source": [
        "## 3.2 Music Tokenizer\n",
        "\n",
        "The `MusicTokenizer` class tokenizes ABC notation into music-aware tokens:\n",
        "- Handles notes, durations, rests, and bar lines\n",
        "- Builds vocabulary from training data\n",
        "- Encodes/decodes between ABC strings and token IDs\n",
        "- Supports special tokens for padding, unknown tokens, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tj1799F9JnN",
        "outputId": "5f3a5a7d-90f1-4c7c-c550-f1965032b906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Music tokenizer defined!\n"
          ]
        }
      ],
      "source": [
        "# Music Tokenizer\n",
        "class MusicTokenizer:\n",
        "    \"\"\"Tokenizer for ABC notation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.vocab_size = 0\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "        self.special_tokens = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<START>': 2,\n",
        "            '<END>': 3,\n",
        "            '<SEP>': 4,\n",
        "        }\n",
        "\n",
        "    def build_vocab(self, abc_strings: List[str], min_freq: int = 2):\n",
        "        \"\"\"Build vocabulary from ABC strings.\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        token_counter = Counter()\n",
        "\n",
        "        for abc_str in tqdm(abc_strings, desc=\"Tokenizing for vocab\"):\n",
        "            tokens = self._tokenize_abc(abc_str)\n",
        "            token_counter.update(tokens)\n",
        "\n",
        "        vocab = dict(self.special_tokens)\n",
        "        current_id = len(self.special_tokens)\n",
        "\n",
        "        for token, count in token_counter.items():\n",
        "            if count >= min_freq:\n",
        "                vocab[token] = current_id\n",
        "                current_id += 1\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.token_to_id = vocab\n",
        "        self.id_to_token = {v: k for k, v in vocab.items()}\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"  Special tokens: {len(self.special_tokens)}\")\n",
        "        print(f\"  Regular tokens: {self.vocab_size - len(self.special_tokens)}\")\n",
        "\n",
        "    def _tokenize_abc(self, abc_str: str) -> List[str]:\n",
        "        \"\"\"Tokenize ABC notation string into music-aware tokens.\"\"\"\n",
        "        tokens = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(abc_str):\n",
        "            char = abc_str[i]\n",
        "\n",
        "            if char.isspace():\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if char == '|':\n",
        "                tokens.append('|')\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if char.upper() in 'ABCDEFG':\n",
        "                note_token = char.upper()\n",
        "                i += 1\n",
        "\n",
        "                if i < len(abc_str) and abc_str[i] in '^_':\n",
        "                    note_token += abc_str[i]\n",
        "                    i += 1\n",
        "\n",
        "                while i < len(abc_str) and abc_str[i] in \",'\":\n",
        "                    note_token += abc_str[i]\n",
        "                    i += 1\n",
        "\n",
        "                tokens.append(note_token)\n",
        "                continue\n",
        "\n",
        "            if char.isdigit():\n",
        "                duration = char\n",
        "                i += 1\n",
        "                while i < len(abc_str) and abc_str[i].isdigit():\n",
        "                    duration += abc_str[i]\n",
        "                    i += 1\n",
        "                tokens.append(f\"DUR:{duration}\")\n",
        "                continue\n",
        "\n",
        "            if char == 'z':\n",
        "                tokens.append('z')\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            tokens.append(char)\n",
        "            i += 1\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, abc_str: str) -> List[int]:\n",
        "        \"\"\"Encode ABC string to token IDs.\"\"\"\n",
        "        tokens = self._tokenize_abc(abc_str)\n",
        "        token_ids = []\n",
        "        for token in tokens:\n",
        "            if token in self.token_to_id:\n",
        "                token_ids.append(self.token_to_id[token])\n",
        "            else:\n",
        "                token_ids.append(self.token_to_id['<UNK>'])\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs back to ABC string.\"\"\"\n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id in self.id_to_token:\n",
        "                tokens.append(self.id_to_token[token_id])\n",
        "            else:\n",
        "                tokens.append('<UNK>')\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def save(self, path: Path):\n",
        "        \"\"\"Save tokenizer to disk.\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'vocab': self.vocab,\n",
        "                'token_to_id': self.token_to_id,\n",
        "                'id_to_token': self.id_to_token,\n",
        "                'vocab_size': self.vocab_size,\n",
        "                'special_tokens': self.special_tokens\n",
        "            }, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: Path):\n",
        "        \"\"\"Load tokenizer from disk.\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        tokenizer = cls()\n",
        "        tokenizer.vocab = data['vocab']\n",
        "        tokenizer.token_to_id = data['token_to_id']\n",
        "        tokenizer.id_to_token = data['id_to_token']\n",
        "        tokenizer.vocab_size = data['vocab_size']\n",
        "        tokenizer.special_tokens = data['special_tokens']\n",
        "        return tokenizer\n",
        "\n",
        "print(\"Music tokenizer defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV6j2uax9JnO"
      },
      "source": [
        "## 4. Data Collection\n",
        "\n",
        "This section downloads and extracts the Lakh MIDI Dataset (LMD-matched) to the `/data` folder.\n",
        "\n",
        "**Note**: The dataset is ~1.7GB compressed and ~2-3GB extracted. This may take several minutes to download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXuRfAbV9JnO",
        "outputId": "42f18e8f-10e7-4f74-93ac-4890c3032cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LMD dataset already exists in /Users/sd6201/Documents/Workspace/symbolic-music-llm-scaling-laws/data/lmd_matched\n",
            "  Found 116189 MIDI files\n",
            "\n",
            "============================================================\n",
            "Data Collection Summary:\n",
            "============================================================\n",
            "MIDI files found: 116,189\n",
            "Location: /Users/sd6201/Documents/Workspace/symbolic-music-llm-scaling-laws/data/lmd_matched\n",
            "\n",
            "Limiting to 10000 files for processing\n",
            "\n",
            "✓ Ready to process 10,000 MIDI files\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Data Collection: Download and Extract Lakh MIDI Dataset\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "LMD_URL = \"http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz\"\n",
        "TAR_PATH = DATA_DIR / \"lmd_matched.tar.gz\"\n",
        "\n",
        "def download_with_progress(url: str, destination: Path):\n",
        "    \"\"\"Download file with progress bar.\"\"\"\n",
        "    def reporthook(count, block_size, total_size):\n",
        "        percent = int(count * block_size * 100 / total_size)\n",
        "        print(f\"\\rDownloading... {percent}% ({count * block_size / 1024 / 1024:.1f} MB / {total_size / 1024 / 1024:.1f} MB)\", end='', flush=True)\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, destination, reporthook=reporthook)\n",
        "        print(\"\\n✓ Download complete!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check if dataset already exists\n",
        "if LMD_DIR.exists() and len(list(LMD_DIR.rglob(\"*.mid\"))) > 0:\n",
        "    print(f\"✓ LMD dataset already exists in {LMD_DIR}\")\n",
        "    print(f\"  Found {len(list(LMD_DIR.rglob('*.mid')))} MIDI files\")\n",
        "else:\n",
        "    print(\"LMD dataset not found. Starting download...\")\n",
        "    print(f\"URL: {LMD_URL}\")\n",
        "    print(f\"Destination: {TAR_PATH}\")\n",
        "    print(f\"Note: This is a large file (~1.7GB), download may take 5-15 minutes depending on connection speed.\\n\")\n",
        "\n",
        "    # Download the dataset\n",
        "    if not TAR_PATH.exists():\n",
        "        if download_with_progress(LMD_URL, TAR_PATH):\n",
        "            print(f\"✓ Downloaded to {TAR_PATH}\")\n",
        "        else:\n",
        "            print(\"✗ Download failed. Please check your internet connection and try again.\")\n",
        "            raise RuntimeError(\"Failed to download LMD dataset\")\n",
        "    else:\n",
        "        print(f\"✓ Tar file already exists: {TAR_PATH}\")\n",
        "\n",
        "    # Extract the dataset\n",
        "    print(f\"\\nExtracting {TAR_PATH} to {DATA_DIR}...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(TAR_PATH, 'r:gz') as tar:\n",
        "            # Get total members for progress\n",
        "            members = tar.getmembers()\n",
        "            total = len(members)\n",
        "\n",
        "            # Extract with progress\n",
        "            for i, member in enumerate(members):\n",
        "                tar.extract(member, DATA_DIR)\n",
        "                if (i + 1) % 1000 == 0:\n",
        "                    print(f\"  Extracted {i+1}/{total} files...\", end='\\r', flush=True)\n",
        "\n",
        "            print(f\"\\n✓ Extracted {total} files\")\n",
        "\n",
        "        # The tar file might extract to a folder with a different name\n",
        "        # Check for extracted folders and rename if needed\n",
        "        extracted_folders = [d for d in DATA_DIR.iterdir()\n",
        "                           if d.is_dir() and 'lmd' in d.name.lower() and d != LMD_DIR]\n",
        "\n",
        "        if extracted_folders and not LMD_DIR.exists():\n",
        "            if len(extracted_folders) == 1:\n",
        "                print(f\"Renaming {extracted_folders[0]} to {LMD_DIR}\")\n",
        "                extracted_folders[0].rename(LMD_DIR)\n",
        "\n",
        "        # Clean up tar file to save space (optional - comment out if you want to keep it)\n",
        "        # TAR_PATH.unlink()\n",
        "        # print(f\"✓ Removed tar file to save space\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Extraction failed: {e}\")\n",
        "        raise RuntimeError(f\"Failed to extract dataset: {e}\")\n",
        "\n",
        "# Verify MIDI files are available\n",
        "midi_files = list(LMD_DIR.rglob(\"*.mid\"))\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Data Collection Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"MIDI files found: {len(midi_files):,}\")\n",
        "print(f\"Location: {LMD_DIR}\")\n",
        "\n",
        "if len(midi_files) == 0:\n",
        "    print(\"\\n⚠ WARNING: No MIDI files found!\")\n",
        "    print(\"Please check:\")\n",
        "    print(f\"  1. Extraction completed successfully\")\n",
        "    print(f\"  2. Files are in: {LMD_DIR}\")\n",
        "    print(f\"  3. Directory structure is correct\")\n",
        "    raise RuntimeError(\"No MIDI files found after download/extraction\")\n",
        "\n",
        "# Limit to 1,000 files for this run\n",
        "MAX_FILES = 1000\n",
        "if len(midi_files) > MAX_FILES:\n",
        "    print(f\"\\nLimiting to {MAX_FILES} files for processing\")\n",
        "    midi_files = midi_files[:MAX_FILES]\n",
        "\n",
        "print(f\"\\n✓ Ready to process {len(midi_files):,} MIDI files\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lAqlVrK9JnO"
      },
      "source": [
        "# Part 5: Data Processing - Convert MIDI to ABC Notation\n",
        "\n",
        "This section processes the downloaded MIDI files and converts them to ABC notation format. The code uses parallel processing for efficiency, based on `data-collection-and-preprocessing/process_pipeline.py`.\n",
        "\n",
        "**Processing**: Up to 1,000 MIDI files will be converted to ABC notation using parallel workers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1LX6YrI9JnO",
        "outputId": "9da9ed41-5d16-4c03-e28c-f06e18db6629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting 10000 MIDI files to ABC notation...\n",
            "Using 14 parallel workers with joblib (notebook-friendly)\n",
            "Processing in chunks of 500 files\n",
            "\n",
            "✓ joblib available - using parallel processing\n",
            "Processing chunk 1/20 (files 1-500)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 1/20\n",
            "  Progress: 5.0% | Success: 487 | Failed: 13 | Chunk time: 67.8s\n",
            "  Estimated time remaining: 21.5 minutes\n",
            "\n",
            "Processing chunk 2/20 (files 501-1000)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 2/20\n",
            "  Progress: 10.0% | Success: 985 | Failed: 15 | Chunk time: 201.0s\n",
            "  Estimated time remaining: 40.3 minutes\n",
            "\n",
            "Processing chunk 3/20 (files 1001-1500)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 3/20\n",
            "  Progress: 15.0% | Success: 1482 | Failed: 18 | Chunk time: 241.1s\n",
            "  Estimated time remaining: 48.2 minutes\n",
            "\n",
            "Processing chunk 4/20 (files 1501-2000)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 4/20\n",
            "  Progress: 20.0% | Success: 1976 | Failed: 24 | Chunk time: 249.4s\n",
            "  Estimated time remaining: 50.6 minutes\n",
            "\n",
            "Processing chunk 5/20 (files 2001-2500)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 5/20\n",
            "  Progress: 25.0% | Success: 2465 | Failed: 35 | Chunk time: 238.7s\n",
            "  Estimated time remaining: 49.9 minutes\n",
            "\n",
            "Processing chunk 6/20 (files 2501-3000)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 6/20\n",
            "  Progress: 30.0% | Success: 2963 | Failed: 37 | Chunk time: 279.3s\n",
            "  Estimated time remaining: 49.7 minutes\n",
            "\n",
            "Processing chunk 7/20 (files 3001-3500)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 7/20\n",
            "  Progress: 35.0% | Success: 3461 | Failed: 39 | Chunk time: 314.4s\n",
            "  Estimated time remaining: 49.3 minutes\n",
            "\n",
            "Processing chunk 8/20 (files 3501-4000)...\n",
            "  Processing with 14 parallel workers...\n",
            "  ✓ Completed chunk 8/20\n",
            "  Progress: 40.0% | Success: 3958 | Failed: 42 | Chunk time: 303.8s\n",
            "  Estimated time remaining: 47.4 minutes\n",
            "\n",
            "Processing chunk 9/20 (files 4001-4500)...\n",
            "  Processing with 14 parallel workers...\n"
          ]
        }
      ],
      "source": [
        "# Convert MIDI files to ABC notation (PARALLEL PROCESSING with ProcessPoolExecutor)\n",
        "# Note: For Jupyter notebooks, we use a self-contained worker function to avoid pickling issues\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from multiprocessing import cpu_count, Pool\n",
        "import time\n",
        "\n",
        "def convert_single_midi_worker(args):\n",
        "    \"\"\"\n",
        "    Convert a single MIDI file to ABC (for parallel processing).\n",
        "    This function is completely self-contained to avoid pickling issues in notebooks.\n",
        "    \"\"\"\n",
        "    midi_file, output_dir = args\n",
        "\n",
        "    try:\n",
        "        # Import all necessary modules in worker process\n",
        "        import warnings\n",
        "        import io\n",
        "        import contextlib\n",
        "        from pathlib import Path\n",
        "        import music21\n",
        "\n",
        "        # Suppress warnings in worker process\n",
        "        warnings.filterwarnings('ignore')\n",
        "        music21.environment.UserSettings()['warnings'] = 0\n",
        "\n",
        "        # Helper function: duration to ABC\n",
        "        def duration_to_abc(quarter_length):\n",
        "            eighth_notes = quarter_length * 2\n",
        "            eighth_notes = round(eighth_notes * 8) / 8\n",
        "            if eighth_notes <= 0:\n",
        "                return \"\"\n",
        "            elif eighth_notes == 0.5:\n",
        "                return \"/\"\n",
        "            elif eighth_notes == 1.0:\n",
        "                return \"\"\n",
        "            elif eighth_notes == 2.0:\n",
        "                return \"2\"\n",
        "            elif eighth_notes == 3.0:\n",
        "                return \"3\"\n",
        "            elif eighth_notes == 4.0:\n",
        "                return \"4\"\n",
        "            elif eighth_notes == 6.0:\n",
        "                return \"6\"\n",
        "            elif eighth_notes == 8.0:\n",
        "                return \"8\"\n",
        "            else:\n",
        "                dur_int = int(eighth_notes)\n",
        "                if dur_int > 0 and dur_int <= 16:\n",
        "                    return str(dur_int)\n",
        "                else:\n",
        "                    return f\"/{int(1/eighth_notes)}\" if eighth_notes < 1 else str(int(eighth_notes))\n",
        "\n",
        "        # Helper function: note to ABC\n",
        "        def note_to_abc(note):\n",
        "            try:\n",
        "                note_name = note.pitch.name[0]\n",
        "                if note.pitch.accidental:\n",
        "                    if note.pitch.accidental.alter == 1:\n",
        "                        note_name = \"^\" + note_name\n",
        "                    elif note.pitch.accidental.alter == -1:\n",
        "                        note_name = \"_\" + note_name\n",
        "                octave = note.pitch.octave\n",
        "                if octave < 4:\n",
        "                    note_name = note_name.lower() * (4 - octave)\n",
        "                elif octave > 4:\n",
        "                    note_name = note_name + \"'\" * (octave - 4)\n",
        "                dur = duration_to_abc(note.duration.quarterLength)\n",
        "                return note_name + dur\n",
        "            except Exception:\n",
        "                return \"\"\n",
        "\n",
        "        # Parse MIDI file\n",
        "        null_stderr = io.StringIO()\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            with contextlib.redirect_stderr(null_stderr):\n",
        "                score = music21.converter.parse(str(midi_file))\n",
        "\n",
        "        # Convert to ABC manually\n",
        "        abc_lines = []\n",
        "        abc_lines.append(\"X:1\")\n",
        "        abc_lines.append(\"M:4/4\")\n",
        "        abc_lines.append(\"L:1/8\")\n",
        "        abc_lines.append(\"K:C\")\n",
        "\n",
        "        # Extract time signature if available\n",
        "        for ts in score.flat.getElementsByClass('TimeSignature'):\n",
        "            if ts.numerator and ts.denominator:\n",
        "                abc_lines[1] = f\"M:{ts.numerator}/{ts.denominator}\"\n",
        "                break\n",
        "\n",
        "        # Extract key signature if available\n",
        "        try:\n",
        "            key = score.analyze('key')\n",
        "            if key:\n",
        "                key_name = key.tonic.name\n",
        "                mode = 'maj' if key.mode == 'major' else 'min'\n",
        "                abc_lines[3] = f\"K:{key_name}{mode[0]}\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Convert notes to ABC body\n",
        "        abc_body = []\n",
        "        measure_count = 0\n",
        "\n",
        "        for element in score.flat.notesAndRests:\n",
        "            if isinstance(element, music21.note.Note):\n",
        "                abc_body.append(note_to_abc(element))\n",
        "            elif isinstance(element, music21.note.Rest):\n",
        "                dur = duration_to_abc(element.duration.quarterLength)\n",
        "                abc_body.append(\"z\" + dur)\n",
        "            elif isinstance(element, music21.chord.Chord):\n",
        "                if len(element.notes) > 0:\n",
        "                    abc_body.append(note_to_abc(element.notes[0]))\n",
        "\n",
        "            measure_count += 1\n",
        "            if measure_count % 4 == 0:\n",
        "                abc_body.append(\"|\")\n",
        "\n",
        "        body_str = \"\".join(abc_body)\n",
        "        abc_lines.append(body_str)\n",
        "        abc_str = \"\\n\".join(abc_lines) if abc_lines else \"\"\n",
        "\n",
        "        if abc_str:\n",
        "            # Clean ABC string\n",
        "            lines = abc_str.split('\\n')\n",
        "            cleaned_lines = []\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if line and not line.startswith('%'):\n",
        "                    cleaned_lines.append(line)\n",
        "            abc_str = '\\n'.join(cleaned_lines)\n",
        "\n",
        "            if len(abc_str.strip()) > 0:\n",
        "                # Save ABC file\n",
        "                abc_path = Path(output_dir) / \"abc\" / f\"{Path(midi_file).stem}.abc\"\n",
        "                abc_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                try:\n",
        "                    with open(abc_path, 'w') as f:\n",
        "                        f.write(abc_str)\n",
        "\n",
        "                    # Verify file was written\n",
        "                    if abc_path.exists() and abc_path.stat().st_size > 0:\n",
        "                        return (str(midi_file), abc_str)\n",
        "                except Exception as e:\n",
        "                    return None\n",
        "\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "    finally:\n",
        "        # Force garbage collection\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# For notebooks, we'll use a simpler approach that works with multiprocessing\n",
        "# The worker function is already self-contained, so we can use it directly\n",
        "\n",
        "\n",
        "# Setup parallel processing using joblib (works better in notebooks)\n",
        "NUM_WORKERS = 14  # Use 14 parallel workers\n",
        "CHUNK_SIZE = 500  # Process in chunks to manage memory\n",
        "\n",
        "print(f\"Converting {len(midi_files)} MIDI files to ABC notation...\")\n",
        "print(f\"Using {NUM_WORKERS} parallel workers with joblib (notebook-friendly)\")\n",
        "print(f\"Processing in chunks of {CHUNK_SIZE} files\\n\")\n",
        "\n",
        "# Try to use joblib for parallel processing (works better in notebooks)\n",
        "try:\n",
        "    from joblib import Parallel, delayed\n",
        "    USE_JOBLIB = True\n",
        "    print(\"✓ joblib available - using parallel processing\")\n",
        "except ImportError:\n",
        "    USE_JOBLIB = False\n",
        "    print(\"⚠ joblib not available - using sequential processing\")\n",
        "    print(\"  Install with: pip install joblib\")\n",
        "\n",
        "# Prepare arguments for processing\n",
        "args_list = [(str(midi_file), str(OUTPUT_DIR)) for midi_file in midi_files]\n",
        "\n",
        "# Process in chunks to prevent memory buildup\n",
        "abc_data = []\n",
        "conversion_stats = {'success': 0, 'failed': 0}\n",
        "\n",
        "total_chunks = (len(args_list) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "start_time = time.time()\n",
        "\n",
        "for chunk_idx in range(total_chunks):\n",
        "    start_idx = chunk_idx * CHUNK_SIZE\n",
        "    end_idx = min(start_idx + CHUNK_SIZE, len(args_list))\n",
        "    chunk_args = args_list[start_idx:end_idx]\n",
        "\n",
        "    print(f\"Processing chunk {chunk_idx + 1}/{total_chunks} (files {start_idx + 1}-{end_idx})...\")\n",
        "    chunk_start_time = time.time()\n",
        "\n",
        "    chunk_results = []\n",
        "\n",
        "    # Process chunk - use joblib for parallel processing if available\n",
        "    if USE_JOBLIB:\n",
        "        # Use joblib.Parallel which handles notebooks better\n",
        "        try:\n",
        "            print(f\"  Processing with {NUM_WORKERS} parallel workers...\")\n",
        "            # joblib.Parallel with verbose=1 shows progress, but we'll use tqdm wrapper\n",
        "            def process_with_progress(args_list):\n",
        "                \"\"\"Process with progress tracking.\"\"\"\n",
        "                results = []\n",
        "                for args in tqdm(args_list, desc=f\"Chunk {chunk_idx + 1}/{total_chunks}\", leave=False):\n",
        "                    results.append(convert_single_midi_worker(args))\n",
        "                return results\n",
        "\n",
        "            # Use joblib for parallel processing\n",
        "            chunk_results = Parallel(n_jobs=NUM_WORKERS, backend='loky', verbose=0)(\n",
        "                delayed(convert_single_midi_worker)(args) for args in chunk_args\n",
        "            )\n",
        "            print(f\"  ✓ Completed chunk {chunk_idx + 1}/{total_chunks}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  joblib parallel failed ({e}), falling back to sequential\")\n",
        "            USE_JOBLIB = False\n",
        "            # Fall through to sequential processing\n",
        "\n",
        "    if not USE_JOBLIB:\n",
        "        # Sequential processing (optimized)\n",
        "        converter = MIDIToABCConverter()\n",
        "        chunk_results = []\n",
        "\n",
        "        for midi_file_str, output_dir_str in tqdm(chunk_args, desc=f\"Chunk {chunk_idx + 1}/{total_chunks}\"):\n",
        "            midi_file = Path(midi_file_str)\n",
        "            abc_str = converter.convert_midi_to_abc(midi_file)\n",
        "\n",
        "            if abc_str:\n",
        "                abc_path = Path(output_dir_str) / \"abc\" / f\"{midi_file.stem}.abc\"\n",
        "                abc_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                try:\n",
        "                    with open(abc_path, 'w') as f:\n",
        "                        f.write(abc_str)\n",
        "                    if abc_path.exists() and abc_path.stat().st_size > 0:\n",
        "                        chunk_results.append((str(midi_file), abc_str))\n",
        "                    else:\n",
        "                        chunk_results.append(None)\n",
        "                except Exception as e:\n",
        "                    chunk_results.append(None)\n",
        "            else:\n",
        "                chunk_results.append(None)\n",
        "\n",
        "            # Periodic garbage collection every 50 files\n",
        "            if len(chunk_results) % 50 == 0:\n",
        "                gc.collect()\n",
        "\n",
        "    # Collect results\n",
        "    for result in chunk_results:\n",
        "        if result is not None:\n",
        "            abc_data.append(result)\n",
        "            conversion_stats['success'] += 1\n",
        "        else:\n",
        "            conversion_stats['failed'] += 1\n",
        "\n",
        "    # Force garbage collection between chunks\n",
        "    gc.collect()\n",
        "\n",
        "    # Print progress\n",
        "    chunk_time = time.time() - chunk_start_time\n",
        "    progress = (chunk_idx + 1) / total_chunks * 100\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_time_per_file = elapsed_time / (conversion_stats['success'] + conversion_stats['failed']) if (conversion_stats['success'] + conversion_stats['failed']) > 0 else 0\n",
        "    remaining_files = len(args_list) - (conversion_stats['success'] + conversion_stats['failed'])\n",
        "    estimated_remaining = remaining_files * avg_time_per_file if avg_time_per_file > 0 else 0\n",
        "\n",
        "    print(f\"  Progress: {progress:.1f}% | \"\n",
        "          f\"Success: {conversion_stats['success']} | \"\n",
        "          f\"Failed: {conversion_stats['failed']} | \"\n",
        "          f\"Chunk time: {chunk_time:.1f}s\")\n",
        "    if estimated_remaining > 0:\n",
        "        print(f\"  Estimated time remaining: {estimated_remaining/60:.1f} minutes\\n\")\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Conversion complete!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Successful: {conversion_stats['success']:,}\")\n",
        "print(f\"  Failed: {conversion_stats['failed']:,}\")\n",
        "if len(midi_files) > 0:\n",
        "    print(f\"  Success rate: {conversion_stats['success'] / len(midi_files) * 100:.1f}%\")\n",
        "print(f\"  Total ABC files: {len(abc_data):,}\")\n",
        "print(f\"  Total time: {total_time/60:.1f} minutes ({total_time:.1f} seconds)\")\n",
        "if conversion_stats['success'] > 0:\n",
        "    print(f\"  Average time per file: {total_time / len(midi_files):.2f} seconds\")\n",
        "    print(f\"  Files per second: {len(midi_files) / total_time:.2f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Convert back to Path objects for consistency\n",
        "abc_data = [(Path(midi_path), abc_str) for midi_path, abc_str in abc_data]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul6X6j8v9JnP"
      },
      "source": [
        "# Part 6: Build Vocabulary and Tokenize\n",
        "\n",
        "This section builds the vocabulary from ABC strings and tokenizes all sequences. Based on `data-collection-and-preprocessing/tokenizer.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1DEbjwO9JnP"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary from ABC strings\n",
        "print(\"Building vocabulary...\")\n",
        "tokenizer = MusicTokenizer()\n",
        "abc_strings = [abc_str for _, abc_str in abc_data]\n",
        "tokenizer.build_vocab(abc_strings, min_freq=2)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save(OUTPUT_DIR / \"tokenizer.pkl\")\n",
        "print(f\"Tokenizer saved to {OUTPUT_DIR / 'tokenizer.pkl'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDZFJNx29JnP"
      },
      "source": [
        "# Part 7: Filter Sequences and Create Splits\n",
        "\n",
        "This section filters sequences by length and creates train/validation/test splits. Based on `data-collection-and-preprocessing/process_pipeline.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGK_psXL9JnP"
      },
      "outputs": [],
      "source": [
        "# Filter sequences by length and tokenize\n",
        "MIN_SEQUENCE_LENGTH = 50\n",
        "MAX_SEQUENCE_LENGTH = 5000\n",
        "\n",
        "print(f\"Filtering sequences (length: {MIN_SEQUENCE_LENGTH}-{MAX_SEQUENCE_LENGTH} tokens)...\")\n",
        "\n",
        "filtered_data = []\n",
        "stats = {'too_short': 0, 'too_long': 0, 'valid': 0}\n",
        "\n",
        "for midi_path, abc_str in tqdm(abc_data, desc=\"Filtering and tokenizing\"):\n",
        "    token_ids = tokenizer.encode(abc_str)\n",
        "    seq_length = len(token_ids)\n",
        "\n",
        "    if seq_length < MIN_SEQUENCE_LENGTH:\n",
        "        stats['too_short'] += 1\n",
        "        continue\n",
        "    elif seq_length > MAX_SEQUENCE_LENGTH:\n",
        "        stats['too_long'] += 1\n",
        "        continue\n",
        "    else:\n",
        "        stats['valid'] += 1\n",
        "        filtered_data.append((midi_path, abc_str, token_ids))\n",
        "\n",
        "print(f\"\\nFiltering statistics:\")\n",
        "print(f\"  Too short (<{MIN_SEQUENCE_LENGTH}): {stats['too_short']}\")\n",
        "print(f\"  Too long (>{MAX_SEQUENCE_LENGTH}): {stats['too_long']}\")\n",
        "print(f\"  Valid: {stats['valid']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWIKkYdj9JnP"
      },
      "outputs": [],
      "source": [
        "# Create train/val/test splits\n",
        "TRAIN_SPLIT = 0.98\n",
        "VAL_SPLIT = 0.01\n",
        "TEST_SPLIT = 0.01\n",
        "\n",
        "print(\"Creating train/val/test splits...\")\n",
        "\n",
        "# Shuffle data\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(filtered_data))\n",
        "filtered_data = [filtered_data[i] for i in indices]\n",
        "\n",
        "# Calculate split indices\n",
        "n_total = len(filtered_data)\n",
        "n_train = int(n_total * TRAIN_SPLIT)\n",
        "n_val = int(n_total * VAL_SPLIT)\n",
        "\n",
        "train_data = filtered_data[:n_train]\n",
        "val_data = filtered_data[n_train:n_train + n_val]\n",
        "test_data = filtered_data[n_train + n_val:]\n",
        "\n",
        "print(f\"  Train: {len(train_data)} sequences\")\n",
        "print(f\"  Val: {len(val_data)} sequences\")\n",
        "print(f\"  Test: {len(test_data)} sequences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNtrnnds9JnP"
      },
      "outputs": [],
      "source": [
        "# Save splits to disk\n",
        "print(\"Saving splits...\")\n",
        "\n",
        "def save_split(data, split_name):\n",
        "    split_dir = OUTPUT_DIR / \"tokenized\" / split_name\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    json_data = []\n",
        "    for midi_path, abc_str, token_ids in data:\n",
        "        json_data.append({\n",
        "            'midi_path': str(midi_path),\n",
        "            'abc': abc_str,\n",
        "            'tokens': token_ids,\n",
        "            'length': len(token_ids)\n",
        "        })\n",
        "\n",
        "    with open(split_dir / \"data.json\", 'w') as f:\n",
        "        json.dump(json_data, f, indent=2)\n",
        "\n",
        "save_split(train_data, \"train\")\n",
        "save_split(val_data, \"val\")\n",
        "save_split(test_data, \"test\")\n",
        "\n",
        "# Calculate statistics\n",
        "train_tokens = sum(len(tokens) for _, _, tokens in train_data)\n",
        "val_tokens = sum(len(tokens) for _, _, tokens in val_data)\n",
        "test_tokens = sum(len(tokens) for _, _, tokens in test_data)\n",
        "\n",
        "print(f\"\\nToken counts:\")\n",
        "print(f\"  Train: {train_tokens:,} tokens ({train_tokens/1e6:.1f}M)\")\n",
        "print(f\"  Val: {val_tokens:,} tokens\")\n",
        "print(f\"  Test: {test_tokens:,} tokens\")\n",
        "print(f\"  Total: {train_tokens + val_tokens + test_tokens:,} tokens\")\n",
        "\n",
        "print(f\"\\n✓ Data preprocessing complete!\")\n",
        "print(f\"  Tokenizer: {OUTPUT_DIR / 'tokenizer.pkl'}\")\n",
        "print(f\"  Train data: {OUTPUT_DIR / 'tokenized' / 'train' / 'data.json'}\")\n",
        "print(f\"  Val data: {OUTPUT_DIR / 'tokenized' / 'val' / 'data.json'}\")\n",
        "print(f\"  Test data: {OUTPUT_DIR / 'tokenized' / 'test' / 'data.json'}\")\n",
        "\n",
        "# Initialize scaling results storage\n",
        "scaling_results = {\n",
        "    'transformer': [],\n",
        "    'rnn': []\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otT3zqc-9JnP"
      },
      "source": [
        "# Part 8: Data Loading Utilities\n",
        "\n",
        "This section defines data loading classes for efficient batching during training. Based on `utils/data_loader.py`.\n",
        "\n",
        "The `MusicDataset` class loads tokenized sequences from JSON files, and `MusicDataLoader` creates batches based on token count (not sequence count) for efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDEuyK6u9JnP"
      },
      "outputs": [],
      "source": [
        "# Data loading classes\n",
        "class MusicDataset(Dataset):\n",
        "    \"\"\"Dataset for tokenized music sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: Path, max_seq_length: int = 5000):\n",
        "        self.data_path = data_path\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.sequences = []\n",
        "\n",
        "        print(f\"Loading sequences from {data_path}...\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            try:\n",
        "                data = json.loads(content)\n",
        "                if isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        token_ids = item.get('token_ids') or item.get('tokens', [])\n",
        "                        if len(token_ids) > max_seq_length:\n",
        "                            token_ids = token_ids[:max_seq_length]\n",
        "                        if len(token_ids) > 0:\n",
        "                            self.sequences.append(token_ids)\n",
        "            except json.JSONDecodeError:\n",
        "                f.seek(0)\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        data = json.loads(line)\n",
        "                        token_ids = data.get('token_ids') or data.get('tokens', [])\n",
        "                        if len(token_ids) > max_seq_length:\n",
        "                            token_ids = token_ids[:max_seq_length]\n",
        "                        if len(token_ids) > 0:\n",
        "                            self.sequences.append(token_ids)\n",
        "\n",
        "        print(f\"Loaded {len(self.sequences)} sequences\")\n",
        "        if len(self.sequences) > 0:\n",
        "            avg_len = sum(len(s) for s in self.sequences) / len(self.sequences)\n",
        "            print(f\"Average sequence length: {avg_len:.1f}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        tokens = torch.tensor(sequence, dtype=torch.long)\n",
        "        input_ids = tokens[:-1]\n",
        "        target_ids = tokens[1:]\n",
        "        return input_ids, target_ids\n",
        "\n",
        "\n",
        "class MusicDataLoader:\n",
        "    \"\"\"Data loader that batches by tokens (not sequences).\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: Path, batch_size_tokens: int,\n",
        "                 max_seq_length: int = 5000, shuffle: bool = True):\n",
        "        self.data_path = data_path\n",
        "        self.batch_size_tokens = batch_size_tokens\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.dataset = MusicDataset(data_path, max_seq_length)\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=0,\n",
        "            collate_fn=lambda batch: batch[0]\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Create batches based on token count.\"\"\"\n",
        "        batch_inputs = []\n",
        "        batch_targets = []\n",
        "        current_batch_tokens = 0\n",
        "\n",
        "        for input_ids, target_ids in self.dataloader:\n",
        "            seq_len = input_ids.size(0)\n",
        "\n",
        "            if current_batch_tokens + seq_len > self.batch_size_tokens and len(batch_inputs) > 0:\n",
        "                max_len = max(seq.size(0) for seq in batch_inputs)\n",
        "                padded_inputs = []\n",
        "                padded_targets = []\n",
        "\n",
        "                for inp, tgt in zip(batch_inputs, batch_targets):\n",
        "                    pad_len = max_len - inp.size(0)\n",
        "                    if pad_len > 0:\n",
        "                        inp = torch.cat([inp, torch.full((pad_len,), -1, dtype=inp.dtype)])\n",
        "                        tgt = torch.cat([tgt, torch.full((pad_len,), -1, dtype=tgt.dtype)])\n",
        "                    padded_inputs.append(inp)\n",
        "                    padded_targets.append(tgt)\n",
        "\n",
        "                yield torch.stack(padded_inputs), torch.stack(padded_targets)\n",
        "\n",
        "                batch_inputs = []\n",
        "                batch_targets = []\n",
        "                current_batch_tokens = 0\n",
        "\n",
        "            batch_inputs.append(input_ids)\n",
        "            batch_targets.append(target_ids)\n",
        "            current_batch_tokens += seq_len\n",
        "\n",
        "        if len(batch_inputs) > 0:\n",
        "            max_len = max(seq.size(0) for seq in batch_inputs)\n",
        "            padded_inputs = []\n",
        "            padded_targets = []\n",
        "\n",
        "            for inp, tgt in zip(batch_inputs, batch_targets):\n",
        "                pad_len = max_len - inp.size(0)\n",
        "                if pad_len > 0:\n",
        "                    inp = torch.cat([inp, torch.full((pad_len,), -1, dtype=inp.dtype)])\n",
        "                    tgt = torch.cat([tgt, torch.full((pad_len,), -1, dtype=tgt.dtype)])\n",
        "                padded_inputs.append(inp)\n",
        "                padded_targets.append(tgt)\n",
        "\n",
        "            yield torch.stack(padded_inputs), torch.stack(padded_targets)\n",
        "\n",
        "print(\"Data loading utilities defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJHeZZQt9JnP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uEDQCjp9JnP"
      },
      "source": [
        "# Part 9: Training Utilities\n",
        "\n",
        "This section defines training functions for model training and evaluation. Based on `train.py` and `utils/training.py`.\n",
        "\n",
        "Functions include:\n",
        "- `get_lr_schedule`: Creates learning rate schedules (cosine annealing with optional warmup)\n",
        "- `train_one_epoch`: Trains model for one epoch with logging\n",
        "- `evaluate`: Evaluates model on validation/test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy4nnCzk9JnQ"
      },
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def get_lr_schedule(optimizer, num_steps, warmup_steps=0):\n",
        "    \"\"\"Create cosine annealing learning rate schedule.\"\"\"\n",
        "    if warmup_steps > 0:\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (num_steps - warmup_steps)\n",
        "                return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    else:\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, scheduler, device, log_interval=100):\n",
        "    \"\"\"Train model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_steps = 0\n",
        "\n",
        "    for step, (input_ids, target_ids) in enumerate(train_loader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # Clamp token IDs to valid range\n",
        "        vocab_size = model.vocab_size\n",
        "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
        "        target_ids = torch.clamp(target_ids, 0, vocab_size - 1)\n",
        "        input_ids = torch.where(input_ids == -1, torch.tensor(0, device=device), input_ids)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, loss = model(input_ids, target_ids)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_steps += 1\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            avg_loss = total_loss / num_steps\n",
        "            current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']\n",
        "            print(f\"Step {step:6d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "    avg_loss = total_loss / num_steps\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_steps = 0\n",
        "\n",
        "    for input_ids, target_ids in val_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        vocab_size = model.vocab_size\n",
        "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
        "        target_ids = torch.clamp(target_ids, 0, vocab_size - 1)\n",
        "        input_ids = torch.where(input_ids == -1, torch.tensor(0, device=device), input_ids)\n",
        "\n",
        "        logits, loss = model(input_ids, target_ids)\n",
        "        total_loss += loss.item()\n",
        "        num_steps += 1\n",
        "\n",
        "    avg_loss = total_loss / num_steps if num_steps > 0 else float('inf')\n",
        "    return avg_loss\n",
        "\n",
        "print(\"Training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh9608ko9JnQ"
      },
      "source": [
        "# Part 10: Initialize Model and Data Loaders\n",
        "\n",
        "This section loads the tokenizer, sets up the device, and initializes a model for training. You can modify the model configuration to train different model sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9MR1fey9JnQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-dM6f9f9JnQ"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = MusicTokenizer.load(OUTPUT_DIR / \"tokenizer.pkl\")\n",
        "print(f\"Tokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model configuration (Tiny model for quick testing)\n",
        "# You can modify these for different model sizes\n",
        "MODEL_CONFIG = {\n",
        "    'd_model': 128,\n",
        "    'n_layers': 2,\n",
        "    'n_heads': 2,\n",
        "    'd_ff': 512,\n",
        "    'max_seq_length': 5000,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "model = MusicTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    **MODEL_CONFIG\n",
        ").to(device)\n",
        "\n",
        "num_params = model.count_parameters()\n",
        "print(f\"\\nModel initialized:\")\n",
        "print(f\"  Parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "print(f\"  Config: {MODEL_CONFIG}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZmC0JhI9JnQ"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_path = OUTPUT_DIR / \"tokenized\" / \"train\" / \"data.json\"\n",
        "val_path = OUTPUT_DIR / \"tokenized\" / \"val\" / \"data.json\"\n",
        "\n",
        "BATCH_SIZE_TOKENS = 50000  # Target tokens per batch\n",
        "MAX_SEQ_LENGTH = MODEL_CONFIG['max_seq_length']\n",
        "\n",
        "train_loader = MusicDataLoader(\n",
        "    train_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = MusicDataLoader(\n",
        "    val_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Data loaders created:\")\n",
        "print(f\"  Batch size (tokens): {BATCH_SIZE_TOKENS:,}\")\n",
        "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSHbVhsi9JnQ"
      },
      "source": [
        "# Part 11: Train Model\n",
        "\n",
        "This section trains a single model. For the scaling study, see Part 13-14 below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5uZaEje9JnQ"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_EPOCHS = 1  # For scaling laws study, typically train for 1 epoch\n",
        "WARMUP_STEPS = 0\n",
        "LOG_INTERVAL = 100\n",
        "\n",
        "# Estimate number of steps (approximate)\n",
        "# You can adjust this based on your actual data size\n",
        "estimated_steps = len(train_loader) if hasattr(train_loader, '__len__') else 1000\n",
        "\n",
        "# Setup optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = get_lr_schedule(optimizer, estimated_steps, warmup_steps=WARMUP_STEPS)\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Estimated steps per epoch: {estimated_steps}\")\n",
        "print(f\"  Log interval: {LOG_INTERVAL}\")\n",
        "\n",
        "# Train for 1 epoch\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training for {NUM_EPOCHS} epoch(s)...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    train_loss = train_one_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device,\n",
        "        log_interval=LOG_INTERVAL\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training complete!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgs9uvE9JnQ"
      },
      "source": [
        "# Part 12: Save Model and Results\n",
        "\n",
        "Save the trained model and test generation. For full scaling study, proceed to Part 13.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvbJzZ-v9JnQ"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_save_path = OUTPUT_DIR / \"model.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': MODEL_CONFIG,\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_parameters': num_params,\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "# Optional: Test generation\n",
        "print(\"\\nTesting generation...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Create a simple starting sequence (just a few tokens)\n",
        "    start_tokens = torch.tensor([[tokenizer.token_to_id.get('C', 0)]], device=device)\n",
        "    generated = model.generate(start_tokens, max_new_tokens=50, temperature=1.0)\n",
        "\n",
        "    # Decode generated tokens\n",
        "    generated_tokens = generated[0].cpu().tolist()\n",
        "    generated_abc = tokenizer.decode(generated_tokens)\n",
        "    print(f\"Generated ABC (first 50 tokens): {generated_abc[:200]}...\")\n",
        "\n",
        "print(\"\\n✓ All done! You can now:\")\n",
        "print(\"  1. Modify MODEL_CONFIG to train different model sizes\")\n",
        "print(\"  2. Train multiple models and collect validation losses\")\n",
        "print(\"  3. Plot scaling laws: validation loss vs. model size\")\n",
        "print(\"  4. Generate music samples from trained models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTPyo0o9JnQ"
      },
      "source": [
        "# Part 13: Model Configurations for Scaling Study\n",
        "\n",
        "This section defines multiple model sizes for transformer and RNN architectures. Based on `experiments/scaling_study.py`.\n",
        "\n",
        "The configurations range from tiny (~1M parameters) to XL (~100M+ parameters) to study scaling laws.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2qhuOh9JnQ"
      },
      "source": [
        "# Part 14: Transformer Scaling Study\n",
        "\n",
        "This section trains multiple transformer models of varying sizes to analyze scaling laws. Based on `experiments/scaling_study.py`.\n",
        "\n",
        "Each model is trained for 1 epoch with consistent hyperparameters to ensure fair comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_8LDZU99JnQ"
      },
      "outputs": [],
      "source": [
        "# Model configurations for scaling study\n",
        "# Transformer configurations\n",
        "TRANSFORMER_CONFIGS = {\n",
        "    'tiny': {\n",
        "        'd_model': 128,\n",
        "        'n_layers': 2,\n",
        "        'n_heads': 2,\n",
        "        'd_ff': 512,\n",
        "        'max_seq_length': 5000,\n",
        "        'dropout': 0.1,\n",
        "        'target_params': 1e6  # ~1M\n",
        "    },\n",
        "    'small': {\n",
        "        'd_model': 256,\n",
        "        'n_layers': 4,\n",
        "        'n_heads': 4,\n",
        "        'd_ff': 1024,\n",
        "        'max_seq_length': 5000,\n",
        "        'dropout': 0.1,\n",
        "        'target_params': 5e6  # ~5M\n",
        "    },\n",
        "    'medium': {\n",
        "        'd_model': 512,\n",
        "        'n_layers': 6,\n",
        "        'n_heads': 8,\n",
        "        'd_ff': 2048,\n",
        "        'max_seq_length': 5000,\n",
        "        'dropout': 0.1,\n",
        "        'target_params': 20e6  # ~20M\n",
        "    },\n",
        "    'large': {\n",
        "        'd_model': 768,\n",
        "        'n_layers': 8,\n",
        "        'n_heads': 8,\n",
        "        'd_ff': 3072,\n",
        "        'max_seq_length': 5000,\n",
        "        'dropout': 0.1,\n",
        "        'target_params': 50e6  # ~50M\n",
        "    },\n",
        "    'xl': {\n",
        "        'd_model': 1024,\n",
        "        'n_layers': 12,\n",
        "        'n_heads': 12,\n",
        "        'd_ff': 4096,\n",
        "        'max_seq_length': 5000,\n",
        "        'dropout': 0.1,\n",
        "        'target_params': 100e6  # ~100M+\n",
        "    }\n",
        "}\n",
        "\n",
        "# RNN/LSTM configurations (matching parameter counts)\n",
        "RNN_CONFIGS = {\n",
        "    'tiny': {\n",
        "        'd_model': 128,\n",
        "        'n_layers': 2,\n",
        "        'dropout': 0.1,\n",
        "        'max_seq_length': 5000,\n",
        "        'target_params': 1e6\n",
        "    },\n",
        "    'small': {\n",
        "        'd_model': 256,\n",
        "        'n_layers': 3,\n",
        "        'dropout': 0.1,\n",
        "        'max_seq_length': 5000,\n",
        "        'target_params': 5e6\n",
        "    },\n",
        "    'medium': {\n",
        "        'd_model': 512,\n",
        "        'n_layers': 4,\n",
        "        'dropout': 0.1,\n",
        "        'max_seq_length': 5000,\n",
        "        'target_params': 20e6\n",
        "    },\n",
        "    'large': {\n",
        "        'd_model': 768,\n",
        "        'n_layers': 5,\n",
        "        'dropout': 0.1,\n",
        "        'max_seq_length': 5000,\n",
        "        'target_params': 50e6\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Model configurations defined!\")\n",
        "print(f\"Transformer configs: {list(TRANSFORMER_CONFIGS.keys())}\")\n",
        "print(f\"RNN configs: {list(RNN_CONFIGS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLxld-aA9JnQ"
      },
      "source": [
        "# Part 15: RNN/LSTM Scaling Study\n",
        "\n",
        "This section trains multiple RNN/LSTM models of varying sizes for comparison with transformers. Based on `experiments/scaling_study.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSadSADp9JnQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgYFHh0F9JnQ"
      },
      "source": [
        "# Part 16: Scaling Analysis and Power Law Fitting\n",
        "\n",
        "This section analyzes the scaling results and fits power laws to the data. Based on `analysis/plot_scaling.py`.\n",
        "\n",
        "The power law relationship is: L = a * N^(-α) + c, where:\n",
        "- L = validation loss\n",
        "- N = number of parameters\n",
        "- α = scaling exponent (key metric)\n",
        "- a, c = fitting parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jxMiOuG9JnV"
      },
      "outputs": [],
      "source": [
        "# Scaling Study: Train multiple transformer models\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Training hyperparameters (consistent across all models)\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_EPOCHS = 1  # Train for 1 epoch for scaling study\n",
        "BATCH_SIZE_TOKENS = 50000\n",
        "LOG_INTERVAL = 100\n",
        "\n",
        "# Load data once\n",
        "train_path = OUTPUT_DIR / \"tokenized\" / \"train\" / \"data.json\"\n",
        "val_path = OUTPUT_DIR / \"tokenized\" / \"val\" / \"data.json\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRANSFORMER SCALING STUDY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training {len(TRANSFORMER_CONFIGS)} transformer models\")\n",
        "print(f\"Each model will train for {NUM_EPOCHS} epoch(s)\")\n",
        "print(f\"Consistent hyperparameters: LR={LEARNING_RATE}, Batch={BATCH_SIZE_TOKENS:,} tokens\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "transformer_results = []\n",
        "\n",
        "for model_name, config in TRANSFORMER_CONFIGS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name.upper()} Transformer\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = MusicTransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=config['d_model'],\n",
        "        n_layers=config['n_layers'],\n",
        "        n_heads=config['n_heads'],\n",
        "        d_ff=config['d_ff'],\n",
        "        max_seq_length=config['max_seq_length'],\n",
        "        dropout=config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    num_params = model.count_parameters()\n",
        "    print(f\"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "    print(f\"Target: {config['target_params']/1e6:.1f}M\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = MusicDataLoader(\n",
        "        train_path,\n",
        "        batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "        max_seq_length=config['max_seq_length'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = MusicDataLoader(\n",
        "        val_path,\n",
        "        batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "        max_seq_length=config['max_seq_length'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    estimated_steps = len(train_loader) if hasattr(train_loader, '__len__') else 1000\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = get_lr_schedule(optimizer, estimated_steps, warmup_steps=0)\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_loader, optimizer, scheduler, device,\n",
        "            log_interval=LOG_INTERVAL\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Get GPU memory usage\n",
        "    gpu_memory_mb = None\n",
        "    if device.type == 'cuda':\n",
        "        gpu_memory_mb = torch.cuda.max_memory_allocated(device) / 1024**2\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    # Store results\n",
        "    result = {\n",
        "        'model_name': model_name,\n",
        "        'architecture': 'transformer',\n",
        "        'num_parameters': num_params,\n",
        "        'train_loss': train_losses[-1] if train_losses else None,\n",
        "        'val_loss': val_loss,\n",
        "        'training_time_seconds': training_time,\n",
        "        'gpu_memory_mb': gpu_memory_mb,\n",
        "        'config': config\n",
        "    }\n",
        "    transformer_results.append(result)\n",
        "    scaling_results['transformer'].append(result)\n",
        "\n",
        "    print(f\"\\n{model_name.upper()} Results:\")\n",
        "    print(f\"  Parameters: {num_params:,}\")\n",
        "    print(f\"  Train Loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Training Time: {training_time/60:.1f} minutes\")\n",
        "    if gpu_memory_mb:\n",
        "        print(f\"  GPU Memory: {gpu_memory_mb:.1f} MB\")\n",
        "\n",
        "    # Clean up\n",
        "    del model, optimizer, scheduler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRANSFORMER SCALING STUDY COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Trained {len(transformer_results)} models\")\n",
        "for r in transformer_results:\n",
        "    print(f\"  {r['model_name']:8s}: {r['num_parameters']/1e6:6.2f}M params, Val Loss: {r['val_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynAMP-V09JnW"
      },
      "source": [
        "# Part 17: Train Best Model for Sample Generation\n",
        "\n",
        "This section trains the largest (best) model for multiple epochs to generate high-quality music samples. Based on `experiments/sample_generation.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vbHFxT9JnW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9LPOKM69JnW"
      },
      "outputs": [],
      "source": [
        "# RNN Scaling Study: Train multiple LSTM models\n",
        "print(\"=\"*60)\n",
        "print(\"RNN/LSTM SCALING STUDY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training {len(RNN_CONFIGS)} LSTM models\")\n",
        "print(f\"Each model will train for {NUM_EPOCHS} epoch(s)\")\n",
        "print(f\"Consistent hyperparameters: LR={LEARNING_RATE}, Batch={BATCH_SIZE_TOKENS:,} tokens\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_results = []\n",
        "\n",
        "for model_name, config in RNN_CONFIGS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name.upper()} LSTM\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = MusicLSTM(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=config['d_model'],\n",
        "        n_layers=config['n_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        max_seq_length=config['max_seq_length']\n",
        "    ).to(device)\n",
        "\n",
        "    num_params = model.count_parameters()\n",
        "    print(f\"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "    print(f\"Target: {config['target_params']/1e6:.1f}M\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = MusicDataLoader(\n",
        "        train_path,\n",
        "        batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "        max_seq_length=config['max_seq_length'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = MusicDataLoader(\n",
        "        val_path,\n",
        "        batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "        max_seq_length=config['max_seq_length'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    estimated_steps = len(train_loader) if hasattr(train_loader, '__len__') else 1000\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = get_lr_schedule(optimizer, estimated_steps, warmup_steps=0)\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_loader, optimizer, scheduler, device,\n",
        "            log_interval=LOG_INTERVAL\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Get GPU memory usage\n",
        "    gpu_memory_mb = None\n",
        "    if device.type == 'cuda':\n",
        "        gpu_memory_mb = torch.cuda.max_memory_allocated(device) / 1024**2\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    # Store results\n",
        "    result = {\n",
        "        'model_name': model_name,\n",
        "        'architecture': 'rnn',\n",
        "        'num_parameters': num_params,\n",
        "        'train_loss': train_losses[-1] if train_losses else None,\n",
        "        'val_loss': val_loss,\n",
        "        'training_time_seconds': training_time,\n",
        "        'gpu_memory_mb': gpu_memory_mb,\n",
        "        'config': config\n",
        "    }\n",
        "    rnn_results.append(result)\n",
        "    scaling_results['rnn'].append(result)\n",
        "\n",
        "    print(f\"\\n{model_name.upper()} Results:\")\n",
        "    print(f\"  Parameters: {num_params:,}\")\n",
        "    print(f\"  Train Loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Training Time: {training_time/60:.1f} minutes\")\n",
        "    if gpu_memory_mb:\n",
        "        print(f\"  GPU Memory: {gpu_memory_mb:.1f} MB\")\n",
        "\n",
        "    # Clean up\n",
        "    del model, optimizer, scheduler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RNN SCALING STUDY COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Trained {len(rnn_results)} models\")\n",
        "for r in rnn_results:\n",
        "    print(f\"  {r['model_name']:8s}: {r['num_parameters']/1e6:6.2f}M params, Val Loss: {r['val_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftyh3BPI9JnW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFtoms2U9JnW"
      },
      "outputs": [],
      "source": [
        "# Scaling Plots and Power Law Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def power_law(N, a, alpha, c):\n",
        "    \"\"\"Power law: L = a * N^(-alpha) + c\"\"\"\n",
        "    return a * np.power(N, -alpha) + c\n",
        "\n",
        "def fit_power_law(param_counts, losses):\n",
        "    \"\"\"Fit power law to scaling data.\"\"\"\n",
        "    p0 = [1.0, 0.1, 0.0]  # Initial guess\n",
        "    try:\n",
        "        popt, pcov = curve_fit(power_law, param_counts, losses, p0=p0, maxfev=10000)\n",
        "        a, alpha, c = popt\n",
        "        perr = np.sqrt(np.diag(pcov))\n",
        "        return a, alpha, c, perr\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Power law fitting failed: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Prepare data for plotting\n",
        "transformer_params = np.array([r['num_parameters'] for r in transformer_results])\n",
        "transformer_losses = np.array([r['val_loss'] for r in transformer_results])\n",
        "\n",
        "rnn_params = np.array([r['num_parameters'] for r in rnn_results])\n",
        "rnn_losses = np.array([r['val_loss'] for r in rnn_results])\n",
        "\n",
        "# Fit power laws\n",
        "transformer_fit = fit_power_law(transformer_params, transformer_losses)\n",
        "rnn_fit = fit_power_law(rnn_params, rnn_losses)\n",
        "\n",
        "# Create combined comparison plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Individual scaling plots\n",
        "ax1.set_xscale('log')\n",
        "ax1.scatter(transformer_params / 1e6, transformer_losses,\n",
        "           s=150, alpha=0.7, label='Transformer', color='blue', zorder=3, marker='o')\n",
        "ax1.scatter(rnn_params / 1e6, rnn_losses,\n",
        "           s=150, alpha=0.7, label='RNN/LSTM', color='red', zorder=3, marker='s')\n",
        "\n",
        "# Plot power law fits\n",
        "if transformer_fit[0] is not None:\n",
        "    a, alpha, c, perr = transformer_fit\n",
        "    N_fit = np.logspace(np.log10(transformer_params.min()),\n",
        "                       np.log10(transformer_params.max()), 100)\n",
        "    L_fit = power_law(N_fit, a, alpha, c)\n",
        "    ax1.plot(N_fit / 1e6, L_fit, 'b--', linewidth=2,\n",
        "            label=f'Transformer Fit (α={alpha:.3f}±{perr[1]:.3f})', zorder=2)\n",
        "\n",
        "if rnn_fit[0] is not None:\n",
        "    a, alpha, c, perr = rnn_fit\n",
        "    N_fit = np.logspace(np.log10(rnn_params.min()),\n",
        "                       np.log10(rnn_params.max()), 100)\n",
        "    L_fit = power_law(N_fit, a, alpha, c)\n",
        "    ax1.plot(N_fit / 1e6, L_fit, 'r--', linewidth=2,\n",
        "            label=f'RNN Fit (α={alpha:.3f}±{perr[1]:.3f})', zorder=2)\n",
        "\n",
        "# Annotate points\n",
        "for r in transformer_results:\n",
        "    ax1.annotate(r['model_name'].upper(),\n",
        "                (r['num_parameters']/1e6, r['val_loss']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "for r in rnn_results:\n",
        "    ax1.annotate(r['model_name'].upper(),\n",
        "                (r['num_parameters']/1e6, r['val_loss']),\n",
        "                xytext=(5, -15), textcoords='offset points', fontsize=9)\n",
        "\n",
        "ax1.set_xlabel('Model Size (Million Parameters)', fontsize=12)\n",
        "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
        "ax1.set_title('Scaling Laws: Validation Loss vs Model Size', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3, which='both')\n",
        "ax1.legend(fontsize=10)\n",
        "\n",
        "# Plot 2: Comparison (overlay)\n",
        "ax2.set_xscale('log')\n",
        "ax2.scatter(transformer_params / 1e6, transformer_losses,\n",
        "           s=150, alpha=0.7, label='Transformer', color='blue', zorder=3, marker='o')\n",
        "ax2.scatter(rnn_params / 1e6, rnn_losses,\n",
        "           s=150, alpha=0.7, label='RNN/LSTM', color='red', zorder=3, marker='s')\n",
        "\n",
        "if transformer_fit[0] is not None:\n",
        "    a, alpha, c, perr = transformer_fit\n",
        "    N_fit = np.logspace(np.log10(transformer_params.min()),\n",
        "                       np.log10(transformer_params.max()), 100)\n",
        "    L_fit = power_law(N_fit, a, alpha, c)\n",
        "    ax2.plot(N_fit / 1e6, L_fit, 'b--', linewidth=2, zorder=2)\n",
        "\n",
        "if rnn_fit[0] is not None:\n",
        "    a, alpha, c, perr = rnn_fit\n",
        "    N_fit = np.logspace(np.log10(rnn_params.min()),\n",
        "                       np.log10(rnn_params.max()), 100)\n",
        "    L_fit = power_law(N_fit, a, alpha, c)\n",
        "    ax2.plot(N_fit / 1e6, L_fit, 'r--', linewidth=2, zorder=2)\n",
        "\n",
        "ax2.set_xlabel('Model Size (Million Parameters)', fontsize=12)\n",
        "ax2.set_ylabel('Validation Loss', fontsize=12)\n",
        "ax2.set_title('Architecture Comparison', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3, which='both')\n",
        "ax2.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'scaling_plots.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"Scaling plots saved to: {OUTPUT_DIR / 'scaling_plots.png'}\")\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SCALING ANALYSIS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nTransformer Models: {len(transformer_results)}\")\n",
        "print(f\"  Parameter range: {transformer_params.min()/1e6:.2f}M - {transformer_params.max()/1e6:.2f}M\")\n",
        "print(f\"  Loss range: {transformer_losses.min():.4f} - {transformer_losses.max():.4f}\")\n",
        "if transformer_fit[0] is not None:\n",
        "    a, alpha, c, perr = transformer_fit\n",
        "    print(f\"  Scaling exponent (α): {alpha:.4f} ± {perr[1]:.4f}\")\n",
        "    print(f\"  Power law: L = {a:.4f} * N^(-{alpha:.4f}) + {c:.4f}\")\n",
        "\n",
        "print(f\"\\nRNN/LSTM Models: {len(rnn_results)}\")\n",
        "print(f\"  Parameter range: {rnn_params.min()/1e6:.2f}M - {rnn_params.max()/1e6:.2f}M\")\n",
        "print(f\"  Loss range: {rnn_losses.min():.4f} - {rnn_losses.max():.4f}\")\n",
        "if rnn_fit[0] is not None:\n",
        "    a, alpha, c, perr = rnn_fit\n",
        "    print(f\"  Scaling exponent (α): {alpha:.4f} ± {perr[1]:.4f}\")\n",
        "    print(f\"  Power law: L = {a:.4f} * N^(-{alpha:.4f}) + {c:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_summary = {\n",
        "    'transformer': transformer_results,\n",
        "    'rnn': rnn_results,\n",
        "    'power_law_fits': {\n",
        "        'transformer': {\n",
        "            'a': float(transformer_fit[0]) if transformer_fit[0] is not None else None,\n",
        "            'alpha': float(transformer_fit[1]) if transformer_fit[1] is not None else None,\n",
        "            'c': float(transformer_fit[2]) if transformer_fit[2] is not None else None,\n",
        "            'alpha_error': float(transformer_fit[3][1]) if transformer_fit[3] is not None else None\n",
        "        },\n",
        "        'rnn': {\n",
        "            'a': float(rnn_fit[0]) if rnn_fit[0] is not None else None,\n",
        "            'alpha': float(rnn_fit[1]) if rnn_fit[1] is not None else None,\n",
        "            'c': float(rnn_fit[2]) if rnn_fit[2] is not None else None,\n",
        "            'alpha_error': float(rnn_fit[3][1]) if rnn_fit[3] is not None else None\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / 'scaling_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {OUTPUT_DIR / 'scaling_results.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ery-kAdd9JnW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruwuHKCQ9JnW"
      },
      "outputs": [],
      "source": [
        "# Train best model (largest transformer) for sample generation\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING BEST MODEL FOR SAMPLE GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the largest transformer configuration\n",
        "best_config_name = 'xl' if 'xl' in TRANSFORMER_CONFIGS else max(TRANSFORMER_CONFIGS.keys(),\n",
        "                                                                 key=lambda k: TRANSFORMER_CONFIGS[k]['target_params'])\n",
        "best_config = TRANSFORMER_CONFIGS[best_config_name]\n",
        "\n",
        "print(f\"Training {best_config_name.upper()} model for sample generation...\")\n",
        "print(f\"Config: {best_config}\")\n",
        "\n",
        "# Initialize best model\n",
        "best_model = MusicTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=best_config['d_model'],\n",
        "    n_layers=best_config['n_layers'],\n",
        "    n_heads=best_config['n_heads'],\n",
        "    d_ff=best_config['d_ff'],\n",
        "    max_seq_length=best_config['max_seq_length'],\n",
        "    dropout=best_config['dropout']\n",
        ").to(device)\n",
        "\n",
        "num_params = best_model.count_parameters()\n",
        "print(f\"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = MusicDataLoader(\n",
        "    train_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=best_config['max_seq_length'],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = MusicDataLoader(\n",
        "    val_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=best_config['max_seq_length'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Setup optimizer and scheduler\n",
        "estimated_steps = len(train_loader) if hasattr(train_loader, '__len__') else 1000\n",
        "optimizer = AdamW(best_model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = get_lr_schedule(optimizer, estimated_steps, warmup_steps=0)\n",
        "\n",
        "# Train for multiple epochs (more than scaling study)\n",
        "NUM_EPOCHS_BEST = 3  # Train longer for better samples\n",
        "print(f\"\\nTraining for {NUM_EPOCHS_BEST} epochs...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS_BEST):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_BEST}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    train_loss = train_one_epoch(\n",
        "        best_model, train_loader, optimizer, scheduler, device,\n",
        "        log_interval=LOG_INTERVAL\n",
        "    )\n",
        "\n",
        "    val_loss = evaluate(best_model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Final evaluation on test set\n",
        "test_path = OUTPUT_DIR / \"tokenized\" / \"test\" / \"data.json\"\n",
        "test_loader = MusicDataLoader(\n",
        "    test_path,\n",
        "    batch_size_tokens=BATCH_SIZE_TOKENS,\n",
        "    max_seq_length=best_config['max_seq_length'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loss = evaluate(best_model, test_loader, device)\n",
        "print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Save best model\n",
        "best_model_path = OUTPUT_DIR / \"best_model.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': best_model.state_dict(),\n",
        "    'model_config': best_config,\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_parameters': num_params,\n",
        "    'test_loss': test_loss,\n",
        "}, best_model_path)\n",
        "\n",
        "print(f\"\\nBest model saved to: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhCvANeb9JnW"
      },
      "outputs": [],
      "source": [
        "# Generate music samples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING MUSIC SAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_model.eval()\n",
        "samples_dir = OUTPUT_DIR / \"samples\"\n",
        "samples_dir.mkdir(exist_ok=True)\n",
        "\n",
        "generated_samples = []\n",
        "\n",
        "# Generate multiple samples\n",
        "NUM_SAMPLES = 10\n",
        "MAX_NEW_TOKENS = 500  # Generate 500 tokens per sample\n",
        "\n",
        "print(f\"Generating {NUM_SAMPLES} samples...\")\n",
        "\n",
        "for i in range(NUM_SAMPLES):\n",
        "    # Unconditional generation: start with a random token or special token\n",
        "    start_token_id = tokenizer.token_to_id.get('<START>', 2)  # Use START token if available\n",
        "    if start_token_id not in tokenizer.token_to_id.values():\n",
        "        # If no START token, use a common note token\n",
        "        start_token_id = tokenizer.token_to_id.get('C', 5)  # Start with C note\n",
        "\n",
        "    start_tokens = torch.tensor([[start_token_id]], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = best_model.generate(\n",
        "            start_tokens,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=1.0,\n",
        "            top_k=50  # Top-k sampling for diversity\n",
        "        )\n",
        "\n",
        "    # Decode generated tokens\n",
        "    generated_tokens = generated[0].cpu().tolist()\n",
        "    generated_abc = tokenizer.decode(generated_tokens)\n",
        "\n",
        "    # Save sample\n",
        "    sample_path = samples_dir / f\"sample_{i+1}.abc\"\n",
        "    with open(sample_path, 'w') as f:\n",
        "        f.write(generated_abc)\n",
        "\n",
        "    generated_samples.append({\n",
        "        'sample_id': i+1,\n",
        "        'tokens': generated_tokens,\n",
        "        'abc': generated_abc,\n",
        "        'length': len(generated_tokens)\n",
        "    })\n",
        "\n",
        "    print(f\"Sample {i+1}: {len(generated_tokens)} tokens\")\n",
        "    print(f\"  Saved to: {sample_path}\")\n",
        "    print(f\"  Preview: {generated_abc[:100]}...\")\n",
        "\n",
        "# Save samples metadata\n",
        "with open(samples_dir / \"samples_metadata.json\", 'w') as f:\n",
        "    json.dump(generated_samples, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Generated {NUM_SAMPLES} samples\")\n",
        "print(f\"Samples saved to: {samples_dir}\")\n",
        "\n",
        "# Evaluate sample quality\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try to convert samples to MIDI for validation\n",
        "valid_samples = 0\n",
        "for i, sample in enumerate(generated_samples):\n",
        "    try:\n",
        "        # Try to parse ABC notation (basic validation)\n",
        "        abc_str = sample['abc']\n",
        "        # Check if it has basic ABC structure\n",
        "        if 'X:' in abc_str or len(abc_str.strip()) > 10:\n",
        "            valid_samples += 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"Valid samples (basic syntax check): {valid_samples}/{NUM_SAMPLES} ({valid_samples/NUM_SAMPLES*100:.1f}%)\")\n",
        "print(f\"Average sample length: {np.mean([s['length'] for s in generated_samples]):.1f} tokens\")\n",
        "print(f\"Test set perplexity: {np.exp(test_loss):.2f}\")\n",
        "\n",
        "# Save evaluation results\n",
        "evaluation_results = {\n",
        "    'test_loss': float(test_loss),\n",
        "    'test_perplexity': float(np.exp(test_loss)),\n",
        "    'num_samples': NUM_SAMPLES,\n",
        "    'valid_samples': valid_samples,\n",
        "    'valid_percentage': float(valid_samples/NUM_SAMPLES*100),\n",
        "    'average_sample_length': float(np.mean([s['length'] for s in generated_samples])),\n",
        "    'samples_dir': str(samples_dir)\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / 'evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nEvaluation results saved to: {OUTPUT_DIR / 'evaluation_results.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ByKdnA9JnW"
      },
      "source": [
        "# Part 18: Generate Music Samples\n",
        "\n",
        "This section generates music samples from the trained model and evaluates their quality. Based on `experiments/sample_generation.py`.\n",
        "\n",
        "Samples are generated using top-k sampling for diversity and saved in ABC notation format.\n",
        "\n",
        "# Part 19: Summary and Next Steps\n",
        "\n",
        "This notebook has completed the full pipeline for the symbolic music LLM scaling laws project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBmzsAHN9JnW"
      },
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\"*60)\n",
        "print(\"PROJECT COMPLETE - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n✓ Data Collection and Preprocessing:\")\n",
        "print(f\"  - Processed {len(abc_data):,} MIDI files\")\n",
        "print(f\"  - Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"  - Train/Val/Test splits created\")\n",
        "\n",
        "print(\"\\n✓ Transformer Scaling Study:\")\n",
        "print(f\"  - Trained {len(transformer_results)} transformer models\")\n",
        "for r in transformer_results:\n",
        "    print(f\"    {r['model_name']:8s}: {r['num_parameters']/1e6:6.2f}M params, Val Loss: {r['val_loss']:.4f}\")\n",
        "\n",
        "print(\"\\n✓ RNN Scaling Study:\")\n",
        "print(f\"  - Trained {len(rnn_results)} RNN/LSTM models\")\n",
        "for r in rnn_results:\n",
        "    print(f\"    {r['model_name']:8s}: {r['num_parameters']/1e6:6.2f}M params, Val Loss: {r['val_loss']:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Scaling Analysis:\")\n",
        "if transformer_fit[0] is not None:\n",
        "    a, alpha, c, perr = transformer_fit\n",
        "    print(f\"  - Transformer scaling exponent: α = {alpha:.4f} ± {perr[1]:.4f}\")\n",
        "if rnn_fit[0] is not None:\n",
        "    a, alpha, c, perr = rnn_fit\n",
        "    print(f\"  - RNN scaling exponent: α = {alpha:.4f} ± {perr[1]:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Sample Generation:\")\n",
        "print(f\"  - Generated {NUM_SAMPLES} music samples\")\n",
        "print(f\"  - Test perplexity: {np.exp(test_loss):.2f}\")\n",
        "print(f\"  - Valid samples: {valid_samples}/{NUM_SAMPLES}\")\n",
        "\n",
        "print(\"\\n📁 Output Files:\")\n",
        "print(f\"  - Scaling plots: {OUTPUT_DIR / 'scaling_plots.png'}\")\n",
        "print(f\"  - Scaling results: {OUTPUT_DIR / 'scaling_results.json'}\")\n",
        "print(f\"  - Best model: {OUTPUT_DIR / 'best_model.pt'}\")\n",
        "print(f\"  - Generated samples: {samples_dir}\")\n",
        "print(f\"  - Evaluation results: {OUTPUT_DIR / 'evaluation_results.json'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Next Steps for Report:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Analyze scaling plots and power law fits\")\n",
        "print(\"2. Compare transformer vs RNN scaling behavior\")\n",
        "print(\"3. Evaluate generated samples qualitatively\")\n",
        "print(\"4. Convert ABC samples to MIDI/audio for playback\")\n",
        "print(\"5. Document design decisions and insights\")\n",
        "print(\"6. Write up findings in LaTeX report\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}